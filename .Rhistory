summary(lm.fit2)
poly(x, 2)
lm.fit2 = lm(y~poly(x, 2))
summary(lm.fit2)
lm.fit = lm(y~x)
summary(lm.fit)
set.seed(1)
x = rnorm(n, 0, 1)
eps = rnorm(n, 0, 0.04)
y = -1 + 0.5 * x + eps
plot(x, y)
lm.fit = lm(y~x)
summary(lm.fit)
abline(lm.fit, col='red')
legend(-2, 0, legend='fitted line', col='red', lty=1)
set.seed(1)
x = rnorm(n, 0, 1)
eps = rnorm(n, 0, 0.64)
y = -1 + 0.5 * x + eps
plot(x, y)
lm.fit = lm(y~x)
summary(lm.fit)
abline(lm.fit, col='red')
legend(-2, 0, legend='fitted line', col='red', lty=1)
set.seed(1)
# ---- (a) ----
x = rnorm(n, 0, 1)
# ---- (b) ----
eps = rnorm(n, 0, 0.25)
# ---- (c) ----
y = -1 + 0.5 * x + eps
# length of y: 100
# beta_0 = -1
# beta_1 = 0.5
# ---- (d) ----
plot(x, y)
# ---- (e) ----
lm.fit = lm(y~x)
summary(lm.fit)
# the estimates of beta_0 and beta_1 are close to the true value
# ---- (f) ----
abline(lm.fit, col='red')
legend(-2, 0, legend='fitted line', col='red', lty=1)
# ---- (g) ----
lm.fit2 = lm(y~poly(x, 2))
summary(lm.fit2)
# The quadratic term improves the r-squared only marginally, but the coefficient
# for the quadratic term is not statistically significant
# ---- (h) ----
set.seed(1)
x = rnorm(n, 0, 1)
eps = rnorm(n, 0, 0.04)
y = -1 + 0.5 * x + eps
plot(x, y)
lm.fit2 = lm(y~x)
summary(lm.fit)
abline(lm.fit, col='red')
legend(-2, 0, legend='fitted line', col='red', lty=1)
# With less noise, the model fits better as the r-squared is drastically
# improved.
# ---- (i) ----
set.seed(1)
x = rnorm(n, 0, 1)
eps = rnorm(n, 0, 0.64)
y = -1 + 0.5 * x + eps
plot(x, y)
lm.fit3 = lm(y~x)
summary(lm.fit)
abline(lm.fit, col='red')
legend(-2, 0, legend='fitted line', col='red', lty=1)
# With more noise, the model fits worse as the r-squared is drastically
# decreased.
# ---- (j) ----
confint(lm.fit)
confint(lm.fit2)
confint(lm.fit3)
set.seed(1)
x1 = runif(100)
x2 = 0.5 * x1 + rnorm(100)/10
y = 2 + 2*x1 + 0.3*x2 + rnorm(100)
# y = beta0 + beta1*x1 + beta2*x2 + eps
# beta0 = 2, beta1 = 2, beta2 = 0.3
cor(x1, x2)
plot(x1, x2)
lm.fit = lm(y~x1+x2)
summary(lm.fit)
lm.fit2 = lm(y~x1)
summary(lm.fit2)
lm.fit3 = lm(y~x2)
summary(lm.fit3)
# ---- (g) ----
x1 = c(x1, 0.1)
x2 = c(x2, 0.8)
y = c(y, 6)
lm.fit = lm(y~x1+x2)
summary(lm.fit)
lm.fit2 = lm(y~x1)
summary(lm.fit2)
lm.fit3 = lm(y~x2)
summary(lm.fit3)
lm.fit = lm(y~x1+x2)
summary(lm.fit)
par(mfrow=c(2,2))
plot(lm.fit)
lm.fit2 = lm(y~x1)
summary(lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
lm.fit3 = lm(y~x2)
summary(lm.fit3)
par(mfrow=c(2,2))
plot(lm.fit3)
?Boston
library(MASS)
?Boston
# -------------------- Problem 15 --------------------
library(MASS)
attach(Boston)
for name in names(Boston)[-crim]{
lm.fit = lm(crim~name, Boston)
summary(lm.fit)
}
for (name in names(Boston)[-crim]){
lm.fit = lm(crim~name, Boston)
summary(lm.fit)
}
for (name in names(Boston)[-crim]){
lm.fit = lm(crim~Boston[[name]], Boston)
summary(lm.fit)
}
for (name in names(Boston)[-crim]){
lm.fit = lm(crim~Boston[[name]], Boston)
print(summary(lm.fit))
}
names(Boston)
names(Boston)[-crim]
names(Boston)[-1]
names(Boston)
for (name in names(Boston)[-1]){
lm.fit = lm(crim~Boston[[name]], Boston)
print(summary(lm.fit))
}
names(Boston)
for (name in names(Boston)[-1]){
lm.fit = lm(crim~Boston[[name]], Boston)
print(name)
summary(lm.fit)
}
#
names(Boston)
for (name in names(Boston)[-1]){
lm.fit = lm(crim~Boston[[name]], Boston)
print(name)
print(summary(lm.fit))
}
#
lm.fit = lm(crim~., Boston)
summary(lm.fit)
coef(lm.fit)
coef(lm.fit)['zn']
coef(lm.fit)[['zn']]
lm.fit = lm(crim~zn, Boston)
coef(lm.fit)
coef(lm.fit)[2]
coef(lm.fit)[[2]]
x = vector()
for (name in names(Boston)[-1]){
lm.fit = lm(crim~Boston[[name]], Boston)
x = c(x, coef(lm.fit)[[name]])
}
coef(lm.fit)[[name]]
coef(lm.fit)[name]
coef(lm.fit)
x = vector()
for (name in names(Boston)[-1]){
lm.fit = lm(crim~Boston[[name]], Boston)
x = c(x, coef(lm.fit)[[2]])
}
lm.fit = lm(crim~., Boston)
coef(lm.fit)[2:end]
coef(lm.fit)[[-1]]
coef(lm.fit)
coef(lm.fit)[-[1]]
coef(lm.fit)[[-1]]
coef(lm.fit)[[2:14]]
coef(lm.fit)[[2:13]]
coef(lm.fit)[2:13]
y = coef(lm.fit)[2:end]
ans = coef(lm.fit)
ans
coef(lm.fit[, -1])
coef(lm.fit[-1])
coef(lm.fit)[, -1]
coef(lm.fit)[-1]
coef(lm.fit)[[-1]
]
coef(lm.fit)[[-1]]
coef(lm.fit)[-1]
ans = coef(lm.fit)[-1]
lm.fit = lm(crim~., Boston)
y = coef(lm.fit)[-1]
plot(x, y)
x
y
x = vector()
for (name in names(Boston)[-1]){
lm.fit = lm(crim~Boston[[name]], Boston)
x = c(x, coef(lm.fit)[2])
}
lm.fit = lm(crim~., Boston)
y = coef(lm.fit)[-1]
plot(x, y)
plot(x, y)
lm.fit = lm(crim~., Boston)
y = unname(coef(lm.fit)[-1])
x = vector()
for (name in names(Boston)[-1]){
lm.fit = lm(crim~Boston[[name]], Boston)
x = c(x, coef(lm.fit)[2])
}
lm.fit = lm(crim~., Boston)
y = unname(coef(lm.fit)[-1])
plot(x, y)
x = vector()
for (name in names(Boston)[-1]){
lm.fit = lm(crim~Boston[[name]], Boston)
x = c(x, coef(lm.fit)[[2]])
}
lm.fit = lm(crim~., Boston)
y = unname(coef(lm.fit)[-1])
plot(x, y)
plot(x, y)
par(mfrow=c(1,1))
plot(x, y)
x = vector()
for (name in names(Boston)[-1]){
lm.fit = lm(crim~Boston[[name]], Boston)
x = c(x, coef(lm.fit)[[2]])
}
lm.fit = lm(crim~., Boston)
#y = unname(coef(lm.fit)[-1])
y = coef(lm.fit)[-1]
par(mfrow=c(1,1))
plot(x, y)
x = vector()
for (name in names(Boston)[-1]){
lm.fit = lm(crim~Boston[[name]], Boston)
x = c(x, coef(lm.fit)[[2]])
}
lm.fit = lm(crim~., Boston)
y = unname(coef(lm.fit)[-1])
par(mfrow=c(1,1))
plot(x, y)
for (name in names(Boston)[-1]){
lm.fit = lm(crim~poly(Boston[[name]], 3), Boston)
print(name)
print(summary(lm.fit))
}
for (name in names(Boston)[-1]){
lm.fit = lm(crim~poly(Boston[name], 3), Boston)
print(name)
print(summary(lm.fit))
}
for (name in names(Boston)[-1]){
print(name)
lm.fit = lm(crim~poly(Boston[[name]], 3), Boston)
print(summary(lm.fit))
}
for (name in names(Boston)[-1]){
print(name)
lm.fit = lm(crim~poly(Boston[[name]], 3), Boston)
print(summary(lm.fit))
}
chas
?Boston
names(Boston)[-c('crim', 'chas')]
names(Boston)
'a' == 'a'
for (name in names(Boston)[-c('crim', 'chas')]){
if (name=='crim' | name=='chas') {next}
print(name)
lm.fit = lm(crim~poly(Boston[[name]], 3), Boston)
print(summary(lm.fit))
}
for (name in names(Boston)){
if (name=='crim' | name=='chas') {next}
print(name)
lm.fit = lm(crim~poly(Boston[[name]], 3), Boston)
print(summary(lm.fit))
}
library(ISLR)
attach(Smarket)
Smarket
names(Smarket)
summary(Smarket)
paris(Smarket)
pairs(Smarket)
cor(Smarket[,-9])
plot(Volume)
glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, Smarket, family=binomial)
summary(glm.fit)
coef(glm.fit)
summary(glm.fit)$coef
summary(glm.fit)$coef[,4]
glm.probs = predict(glm.fit, type='response')
glm.probs[1:10,]
glm.probs = predict(glm.fit, type='response')
glm.probs[1:10]
contrasts(Direction)
glm.probs = predict(glm.fit)
glm.probs[1:10]
log(glm.probls[1:10])
log(glm.probs[1:10])
exp(glm.probs[1:10])
glm.probs = predict(glm.fit, type='response')
glm.probs[1:10]
glm.probs.no = predict(glm.fit)
glm.probs[1:10]
glm.probs.no[1:10]
1/(1+exp(glm.probs.no[1:10]))
1 - 1/(1+exp(glm.probs.no[1:10]))
1/(1+exp(-glm.probs.no[1:10]))
nobs(glm.fit)
glm.pred = rep('Down', nobs(glm.fit))
glm.pred[glm.probs > 0.5] = 'Up'
table(glm.pred, Direction)
mean(glm.pred==Direction)
train = (Year<2005)
Smarket.2005 = Smarket[!train]
dim(Smarket.2005)
Direction.2005 = Direction[!train]
train = (Year<2005)
Smarket.2005 = Smarket[!train,]
dim(Smarket.2005)
Direction.2005 = Direction[!train]
glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, family=binomial, subset=train)
glm.probs = predict(glm.fit, type='response')
glm.probs = predict(glm.fit, Smarket.2005, type='response')
glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, family=binomial, subset=train)
glm.probs = predict(glm.fit, Smarket.2005, type='response')
glm.pred = rep('Down', nobs(Smarket.2005))
glm.pred[glm.probs>=0.5] = 'Up'
table(glm.pred, Direction)
mean(glm.pred==Direction)
glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, family=binomial, subset=train)
glm.probs = predict(glm.fit, Smarket.2005, type='response')
glm.pred = rep('Down', nrow(Smarket.2005))
glm.pred[glm.probs>=0.5] = 'Up'
table(glm.pred, Direction)
mean(glm.pred==Direction)
glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, family=binomial, subset=train)
glm.probs = predict(glm.fit, Smarket.2005, type='response')
glm.pred = rep('Down', nrow(Smarket.2005))
glm.pred[glm.probs>=0.5] = 'Up'
table(glm.pred, Direction)
mean(glm.pred==Direction)
nrow(Smarket.2005)
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005)
glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, family=binomial, subset=train)
glm.probs = predict(glm.fit, Smarket.2005, type='response')
glm.pred = rep('Down', nrow(Smarket.2005))
glm.pred[glm.probs>=0.5] = 'Up'
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005)
source('C:/Users/Penghao Chen/Desktop/R/ISLR/RLab/Ch4.R', echo=TRUE)
glm.fit = glm(Direction~Lag1+Lag2, family=binomial, subset=train)
glm.probs = predict(glm.fit, Smarket.2005, type='response')
glm.pred = rep('Down', nrow(Smarket.2005))
glm.pred[glm.probs>=0.5] = 'Up'
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005)
predict(glm.fit, newdata=data.frame(Lag1=c(1.2, 1.5), Lag2=c(1.1, -0.8)), type=binomial)
predict(glm.fit, newdata=data.frame(Lag1=c(1.2, 1.5), Lag2=c(1.1, -0.8)), type='response')
library(MASS)
lda.fit = lda(Direction~Lag1+Lag2, subset=train)
summary(lda.fit)
plot(lda.fit)
lda.fit
lda.pred = predict(lda.fit, type='response')
lda.pred = predict(lda.fit, Smarket.2005, type='response')
names(lda.pred)
lda.class = lda.pred$class
table(lda.class, Direction.2005)
lda.class = lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class==Direction.2005)
lda.pred$posterior
lda.pred$posterior[1:20, ]
contrasts(Direction.2005)
sum(lda.pred$posterior >= 0.5)
sum(lda.pred$posterior[,1] >= 0.5)
sum(lda.pred$posterior[,1] < 0.5)
lda.class[1:20,]
lda.class[1:20]
lda.pred$posterior[1:20]
sum(lda.pred$posterior[,1] >= 0.9)
sum(lda.pred$posterior[,1] < 0.9)
source('C:/Users/Penghao Chen/Desktop/R/ISLR/RLab/Ch4.R', echo=TRUE)
qda.fit = qda(Direction~Lag1+Lag2, subset=train)
qda.fit
qda.pred = predict(qda.fit, Smarket.2005)
qda.pred$class
qda.pred = predict(qda.fit, Smarket.2005)$class
qda.class = predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction)
table(qda.class, Direction.2005)
mean(qda.class==Direction.2005)
source('C:/Users/Penghao Chen/Desktop/R/ISLR/RLab/Ch4.R', echo=TRUE)
Lag1
size(Lag1)
length(Lag1)
library(class)
train.X = cbind(Lag1, Lag2)[train, ]
test.X = cbind(Lag1, Lag2)[!train, ]
train.Direction = Direction[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k=1)
table(knn.pred, Direction.2005)
(43+83)/252
knn.pred = knn(train.X, test.X, train.Direction, k=3)
table(knn.pred, Direction.2005)
(48+87)/252
dim(Caravan)
summary(Purchase)
attach(Caravan)
summary(Purchase)
standardized.X = scale(Caravan[, -ncol(Caravan)])
ncol(Caravan)
names(standardized.X)
class(standardized.X)
class(Caravan)
names(Caravan)
var(standardized.X$)
standardized.X = scale(Caravan[, -ncol(Caravan)])
var(standardized.X$)
var(standardized.X)
var(standardized.X[, 1])
mean(standardized.X[, 1])
test = 1:1000
train.X = standardized.X[-test, ]
test.X = standardized.X[test, ]
train.Y = Caravan[-test, ncol(Caravan)]
test.Y = Caravan[test, ncol(Caravan)]
knn.pred = knn(train.X, test.X, train.Y, k=1)
test = 1:1000
train.X = standardized.X[-test, ]
test.X = standardized.X[test, ]
train.Y = Purchase[-test]
test.Y = Purchase[test]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Y, k=1)
mean(test.Y == knn.pred)
mean(test.Y =!= 'No')
mean(test.Y != 'No')
mean((test.Y == 'Yes') * (knn.pred == 'Yes'))
(test.Y == 'Yes')
(test.Y == 'Yes') * (knn.pred == 'Yes')
sum((test.Y == 'Yes') * (knn.pred == 'Yes')) / sum(test.Y=='Yes')
sum((test.Y == 'Yes') * (knn.pred == 'Yes')) / sum(test.Y=='Yes')
sum((test.Y == 'Yes') * (knn.pred == 'Yes')) / sum(knn.pred=='Yes')
table(knn.pred, test.Y)
9/(68+9)
knn.pred = knn(train.X, test.X, train.Y, k=3)
table(knn.pred, test.Y)
5/26
knn.pred = knn(train.X, test.X, train.Y, k=5)
table(knn.pred, test.Y)
4/15
glm.fit = glm(train.Y~train.X, family=binomial)
glm.fit = glm(Purchase~., family=binomial, subset=-test)
glm.fit = glm(Purchase~., data=Caravan, family=binomial, subset=-test)
glm.pred = predict(glm, test.X, typle='')
glm.pred = predict(glm.fit, test.X, typle='')
glm.fit = glm(Purchase~., data=Caravan, family=binomial, subset=-test)
glm.probs = predict(glm.fit, Caravan, typle='')
glm.pred = rep('No', max(test))
glm.pred[ glm.probs>0.5 ] = 'Yes'
mean(glm.pred==test.Y)
glm.pred
test.Y
length(test.Y)
length(glm.pred)
mean(glm.pred[test]==test.Y)
table(glm.pred, test.Y)
glm.fit = glm(Purchase~., data=Caravan, family=binomial, subset=-test)
glm.probs = predict(glm.fit, Caravan, typle='')[test]
glm.pred = rep('No', max(test))
glm.pred[ glm.probs>0.5 ] = 'Yes'
table(glm.pred, test.Y)
glm.pred[ glm.probs>=0.5 ] = 'Yes'
table(glm.pred, test.Y)
glm.pred[ glm.probs>=0.25 ] = 'Yes'
table(glm.pred, test.Y)
glm.fit = glm(Purchase~., data=Caravan, family=binomial, subset=-test)
glm.probs = predict(glm.fit, Caravan[test], typle='')
glm.probs = predict(glm.fit, Caravan[test, ], typle='')
glm.pred = rep('No', max(test))
glm.pred[ glm.probs>=0.5 ] = 'Yes'
table(glm.pred, test.Y)
max(test)
glm.probs = predict(glm.fit, Caravan[test, ], typle='response')
glm.pred = rep('No', max(test))
glm.pred[ glm.probs>=0.5 ] = 'Yes'
table(glm.pred, test.Y)
glm.fit = glm(Purchase~., data=Caravan, family=binomial, subset=-test)
glm.probs = predict(glm.fit, Caravan[test, ], type='response')
glm.pred = rep('No', max(test))
glm.pred[ glm.probs>=0.5 ] = 'Yes'
table(glm.pred, test.Y)
glm.pred[ glm.probs>=0.25 ] = 'Yes'
table(glm.pred, test.Y)
