boost.test = predict(boost.fit, newdata=test, n.trees=ntree)
err.train = mean( (boost.train-train$logSalary)^2 )
err.test = mean( (boost.test-test$logSalary)^2 )
mse.train = c(mse.train, err.train)
mse.test = c(mse.test, err.test)
}
plot(lambda, mse.train, type='l', xlab='lambda', ylab='Training MSE')
lambda
# -------------------------- (c) --------------------------
ntree = 1000
lambda = seq(0, 1, by=0.05)
mse.train = c()
mse.test = c()
for(l in lambda){
boost.fit = gbm(logSalary~.-Salary, distribution='gaussian', data=train, n.trees=ntree, shrinkage=l)
boost.train = predict(boost.fit, newdata=train, n.trees=ntree)
boost.test = predict(boost.fit, newdata=test, n.trees=ntree)
err.train = mean( (boost.train-train$logSalary)^2 )
err.test = mean( (boost.test-test$logSalary)^2 )
mse.train = c(mse.train, err.train)
mse.test = c(mse.test, err.test)
}
plot(lambda, mse.train, type='l', xlab='lambda', ylab='Training MSE')
plot(lambda, mse.test, type='l', xlab='lambda', ylab='Test MSE')
lambda[which.min(mse.test)]
library(boot)
?cv.glm
library(glm)
?glm
# Cross-validation and Factor selection omitted
lm.fit = lm(logSalary~.-Salary, data=train)
lm.pred = predict(lm.fit, newdata=test)
mse.lm = mean( (lm.pred-test$logSalary)^2 )
mse.lm
library(glmnet)
install.packages("glmnet")
?cv.glm()
?cv.glmnet()
?cv.glmnet
library(glmnet)
cv.glmnet
?cv.glmnet
# Applying ridge regression
x = train[, -c('Salary', 'logSalary')]
# Applying ridge regression
x = train[, ~c('Salary', 'logSalary')]
# Applying ridge regression
x = train[, -.('Salary', 'logSalary')]
# Applying ridge regression
x = train[, -('Salary', 'logSalary')]
# Applying ridge regression
x = train[, -'Salary']
# Applying ridge regression
x = subset(train, select=c('Salary', 'logSalary'))
View(x)
# Applying ridge regression
x = subset(train, select=-c('Salary', 'logSalary'))
# Applying ridge regression
x = subset(train, select=-c(Salary, logSalary))
ridge.cv = cv.glmnet(x, y, nfolds=10)
x = subset(train, select=-c(Salary, logSalary))
y = train$logSalary
ridge.cv = cv.glmnet(x, y, nfolds=10)
ridge.cv = cv.glmnet(x, y)
# Applying ridge regression
x = as.matrix( subset(train, select=-c(Salary, logSalary)) )
View(x)
# Applying ridge regression
x = as.matrix( subset(train, select=-c(Salary, logSalary)) )
y = train$logSalary
ridge.cv = cv.glmnet(x, y)
# Applying ridge regression
x = as.matrix( subset(train, select=-c(Salary, logSalary)) )
y = train$logSalary
ridge.cv = cv.glmnet(x, y)
View(x)
# Applying ridge regression
x = model.matrix(logSalary~.-Salary, data=train)
y = train$logSalary
ridge.cv = cv.glmnet(x, y)
ridge.cv
ridge.cv = cv.glmnet(x, y, nfolds=10)
summary(ridge.cv)
ridge.cv
?glmnet
library(glmnet)
library(boot)
library(gbm)
library(ISLR)
attach(Hitters)
set.seed(1)
# -------------------------- (a) --------------------------
Hitters = Hitters[!is.na(Hitters$Salary), ]
Hitters[['logSalary']] = log(Hitters$Salary)
# -------------------------- (b) --------------------------
idx.train = 1:200
train = Hitters[idx.train, ]
test = Hitters[-idx.train, ]
# -------------------------- (c) --------------------------
ntree = 1000
lambda = seq(0, 1, by=0.05)
mse.train = c()
mse.test = c()
for(l in lambda){
boost.fit = gbm(logSalary~.-Salary, distribution='gaussian', data=train, n.trees=ntree, shrinkage=l)
boost.train = predict(boost.fit, newdata=train, n.trees=ntree)
boost.test = predict(boost.fit, newdata=test, n.trees=ntree)
err.train = mean( (boost.train-train$logSalary)^2 )
err.test = mean( (boost.test-test$logSalary)^2 )
mse.train = c(mse.train, err.train)
mse.test = c(mse.test, err.test)
}
plot(lambda, mse.train, type='l', xlab='lambda', ylab='Training MSE')
# -------------------------- (d) --------------------------
plot(lambda, mse.test, type='l', xlab='lambda', ylab='Test MSE')
# The optimal shrinkage parameter is 0.1
# -------------------------- (e) --------------------------
# Applying linear regression
# Cross-validation and Factor selection omitted
lm.fit = lm(logSalary~.-Salary, data=train)
lm.pred = predict(lm.fit, newdata=test)
mse.lm.test = mean( (lm.pred-test$logSalary)^2 )     # MSE is 0.4917959
# Applying ridge regression
x = model.matrix(logSalary~.-Salary, data=train)
y = train$logSalary
ridge.cv = cv.glmnet(x, y, nfolds=10)
lambda.min = ridge.cv$lambda.min
ridge.fit = glmnet(x, y, family='gaussian', alpha=0, lambda=lambda.min)
ridge.pred = predict(ridge.fit, newdata=test)
mse.ridge.test = mean( (ridge.pred-test$logSalary)^2 )
new.x = model.matrix(logSalary~.-Salary, data=test)
ridge.pred = predict(ridge.fit, newx=new.x)
mse.ridge.test = mean( (ridge.pred-test$logSalary)^2 )
library(glmnet)
library(boot)
library(gbm)
library(ISLR)
attach(Hitters)
set.seed(1)
# -------------------------- (a) --------------------------
Hitters = Hitters[!is.na(Hitters$Salary), ]
Hitters[['logSalary']] = log(Hitters$Salary)
# -------------------------- (b) --------------------------
idx.train = 1:200
train = Hitters[idx.train, ]
test = Hitters[-idx.train, ]
# -------------------------- (c) --------------------------
ntree = 1000
lambda = seq(0, 1, by=0.05)
mse.train = c()
mse.test = c()
for(l in lambda){
boost.fit = gbm(logSalary~.-Salary, distribution='gaussian', data=train, n.trees=ntree, shrinkage=l)
boost.train = predict(boost.fit, newdata=train, n.trees=ntree)
boost.test = predict(boost.fit, newdata=test, n.trees=ntree)
err.train = mean( (boost.train-train$logSalary)^2 )
err.test = mean( (boost.test-test$logSalary)^2 )
mse.train = c(mse.train, err.train)
mse.test = c(mse.test, err.test)
}
plot(lambda, mse.train, type='l', xlab='lambda', ylab='Training MSE')
# -------------------------- (d) --------------------------
plot(lambda, mse.test, type='l', xlab='lambda', ylab='Test MSE')
# The optimal shrinkage parameter is 0.1
# -------------------------- (e) --------------------------
# Applying linear regression
# Cross-validation and Factor selection omitted
lm.fit = lm(logSalary~.-Salary, data=train)
lm.pred = predict(lm.fit, newdata=test)
mse.lm.test = mean( (lm.pred-test$logSalary)^2 )     # MSE is 0.4917959
# Applying ridge regression
x = model.matrix(logSalary~.-Salary, data=train)
y = train$logSalary
ridge.cv = cv.glmnet(x, y, nfolds=10)
lambda.min = ridge.cv$lambda.min
new.x = model.matrix(logSalary~.-Salary, data=test)
ridge.fit = glmnet(x, y, family='gaussian', alpha=0, lambda=lambda.min)
ridge.pred = predict(ridge.fit, newx=new.x)
mse.ridge.test = mean( (ridge.pred-test$logSalary)^2 )
mse.lm.test
mse.ridge.test
min(mse.test)
summary(ridge.fit)
ridge.fit
summary(ridge.fit)
ridge.fit$beta
?importance
library(randomForest)
?importance
?varImpPlot
# -------------------------- (f) --------------------------
lambda.opt = lambda[which.min(mse.test)]
lambda.opt
library(randomForest)
library(glmnet)
library(boot)
library(gbm)
library(ISLR)
attach(Hitters)
set.seed(1)
# -------------------------- (a) --------------------------
Hitters = Hitters[!is.na(Hitters$Salary), ]
Hitters[['logSalary']] = log(Hitters$Salary)
# -------------------------- (b) --------------------------
idx.train = 1:200
train = Hitters[idx.train, ]
test = Hitters[-idx.train, ]
# -------------------------- (c) --------------------------
ntree = 1000
lambda = seq(0, 1, by=0.05)
mse.train = c()
mse.test = c()
for(l in lambda){
boost.fit = gbm(logSalary~.-Salary, distribution='gaussian', data=train, n.trees=ntree, shrinkage=l)
boost.train = predict(boost.fit, newdata=train, n.trees=ntree)
boost.test = predict(boost.fit, newdata=test, n.trees=ntree)
err.train = mean( (boost.train-train$logSalary)^2 )
err.test = mean( (boost.test-test$logSalary)^2 )
mse.train = c(mse.train, err.train)
mse.test = c(mse.test, err.test)
}
plot(lambda, mse.train, type='l', xlab='lambda', ylab='Training MSE')
# -------------------------- (d) --------------------------
plot(lambda, mse.test, type='l', xlab='lambda', ylab='Test MSE')
# The optimal shrinkage parameter is 0.1
# The optimal MSE is 0.255685
# -------------------------- (e) --------------------------
# Applying linear regression
# Cross-validation and Factor selection omitted
lm.fit = lm(logSalary~.-Salary, data=train)
lm.pred = predict(lm.fit, newdata=test)
mse.lm.test = mean( (lm.pred-test$logSalary)^2 )     # MSE is 0.4917959
# Applying ridge regression
x = model.matrix(logSalary~.-Salary, data=train)
y = train$logSalary
ridge.cv = cv.glmnet(x, y, nfolds=10)
lambda.min = ridge.cv$lambda.min
new.x = model.matrix(logSalary~.-Salary, data=test)
ridge.fit = glmnet(x, y, family='gaussian', alpha=0, lambda=lambda.min)
ridge.pred = predict(ridge.fit, newx=new.x)
mse.ridge.test = mean( (ridge.pred-test$logSalary)^2 )   # MSE is 0.4755723
# -------------------------- (f) --------------------------
lambda.opt = lambda[which.min(mse.test)]
boost.opt = gbm(logSalary~.-Salary, distribution='gaussian', data=train, n.trees=ntree, shrinkage=lambda.opt)
lambda.opt
min(mse.test)
boost.opt = gbm(logSalary~.-Salary, distribution='gaussian', data=train, n.trees=ntree, shrinkage=lambda.opt)
importance(boost.opt)
summary(boost.opt)
View(x)
?randomForest
View(train)
# -------------------------- (g) --------------------------
bag.fit = randomForest(logSalary~.-Salary, data=train, mtry=ncol(train)-2)
bag.pred = predict(bag.fit, newdata=test)
mse.bag.test = mean( (bag.pred-test$logSalary)^2 )
mse.bag.test
library(randomForest)
library(glmnet)
library(boot)
library(gbm)
library(ISLR)
attach(Hitters)
set.seed(1)
# -------------------------- (a) --------------------------
Hitters = Hitters[!is.na(Hitters$Salary), ]
Hitters[['logSalary']] = log(Hitters$Salary)
# -------------------------- (b) --------------------------
idx.train = 1:200
train = Hitters[idx.train, ]
test = Hitters[-idx.train, ]
# -------------------------- (c) --------------------------
ntree = 1000
lambda = seq(0, 1, by=0.05)
mse.train = c()
mse.test = c()
for(l in lambda){
boost.fit = gbm(logSalary~.-Salary, distribution='gaussian', data=train, n.trees=ntree, shrinkage=l)
boost.train = predict(boost.fit, newdata=train, n.trees=ntree)
boost.test = predict(boost.fit, newdata=test, n.trees=ntree)
err.train = mean( (boost.train-train$logSalary)^2 )
err.test = mean( (boost.test-test$logSalary)^2 )
mse.train = c(mse.train, err.train)
mse.test = c(mse.test, err.test)
}
plot(lambda, mse.train, type='l', xlab='lambda', ylab='Training MSE')
# -------------------------- (d) --------------------------
plot(lambda, mse.test, type='l', xlab='lambda', ylab='Test MSE')
# The optimal shrinkage parameter is 0.15
# The optimal MSE is 0.255685
# -------------------------- (e) --------------------------
# Applying linear regression
# Cross-validation and Factor selection omitted
lm.fit = lm(logSalary~.-Salary, data=train)
lm.pred = predict(lm.fit, newdata=test)
mse.lm.test = mean( (lm.pred-test$logSalary)^2 )     # MSE is 0.4917959
# Applying ridge regression
x = model.matrix(logSalary~.-Salary, data=train)
y = train$logSalary
ridge.cv = cv.glmnet(x, y, nfolds=10)
lambda.min = ridge.cv$lambda.min
new.x = model.matrix(logSalary~.-Salary, data=test)
ridge.fit = glmnet(x, y, family='gaussian', alpha=0, lambda=lambda.min)
ridge.pred = predict(ridge.fit, newx=new.x)
mse.ridge.test = mean( (ridge.pred-test$logSalary)^2 )   # MSE is 0.4755723
# -------------------------- (f) --------------------------
lambda.opt = lambda[which.min(mse.test)]
boost.opt = gbm(logSalary~.-Salary, distribution='gaussian', data=train, n.trees=ntree, shrinkage=lambda.opt)
summary(boost.opt)
# Variable CAtBat seems the most important variable
# -------------------------- (g) --------------------------
bag.fit = randomForest(logSalary~.-Salary, data=train, mtry=ncol(train)-2)
bag.pred = predict(bag.fit, newdata=test)
mse.bag.test = mean( (bag.pred-test$logSalary)^2 )    # MSE is 0.2329957
mse.bag.test
min(mse.test)
library(ISLR)
attach(Caravan)
?Caravan
library(gbm)
?gbm
# -------------------------- (a) --------------------------
idx.train = 1:1000
train = Caravan[idx.train, ]
test = Caravan[-idx.train, ]
View(train)
# -------------------------- (a) --------------------------
ntree = 1000
lambda = 0.01
boost.fit = gbm(Purchase~., data=train, distribution='bernoulli', n.trees=ntree, shrinkage=lambda)
boost.fit = gbm(Purchase~., data=train, n.trees=ntree, shrinkage=lambda)
View(train)
library(gbm)
library(ISLR)
attach(Caravan)
set.seed(1)
# -------------------------- (a) --------------------------
Caravan$Purchase = ifelse(Caravan$Purchase=='Yes', 1, 0)
idx.train = 1:1000
train = Caravan[idx.train, ]
test = Caravan[-idx.train, ]
# -------------------------- (a) --------------------------
ntree = 1000
lambda = 0.01
boost.fit = gbm(Purchase~., data=train, distribution='bernoulli', n.trees=ntree, shrinkage=lambda)
boost.fit
summary(boost.fit)
# -------------------------- (c) --------------------------
boost.pred = predict(boost.fit, newdata=test, type='response')
# -------------------------- (c) --------------------------
boost.pred = predict(boost.fit, newdata=test, n.trees=ntree, type='response')
boost.pred = ifelse(boost.pred>0.2, 1, 0)
table(pred=boost.pred, actual=test$Purchase)
256/(256+33)
33/(33+123)
library(class)
train.mat = subset(train, select=-'Purchase')
train.mat = subset(train, select=-c('Purchase'))
train.mat = subset(train, select=-Purchase)
?knn
# KNN
k.vec = 1:20
train.mat = subset(train, select=-Purchase)
test.mat = subset(test, select=-Purchase)
knn.pred = knn(train.mat, test.mat, train$Purchase, k=5)
knn.pred
knn.pred = knn(train.mat, test.mat, train$Purchase, k=5, prob=TRUE)
knn.pred
# KNN: Cross-validating k omitted
k = 20
train.mat = subset(train, select=-Purchase)
test.mat = subset(test, select=-Purchase)
knn.pred = knn(train.mat, test.mat, train$Purchase, k=k, prob=TRUE)
knn.pred = ifelse(knn.pred>0.2, 1, 0)
knn.pred = knn(train.mat, test.mat, train$Purchase, k=k, prob=TRUE)
knn.pred = knn(train.mat, test.mat, train$Purchase, k=k, prob=TRUE)
knn.pred
as.numeric(knn.pred)
knn.pred
knn.pred$prd
knn.pred$prob
attr(knn.pred, 'prob)'
)
attr(knn.pred, 'prob')
knn.pred = knn(train.mat, test.mat, train$Purchase, k=k, prob=TRUE)$
knn.pred = attr(knn.pred, 'prob')
library(class)
library(gbm)
library(ISLR)
attach(Caravan)
set.seed(1)
# -------------------------- (a) --------------------------
Caravan$Purchase = ifelse(Caravan$Purchase=='Yes', 1, 0)
idx.train = 1:1000
train = Caravan[idx.train, ]
test = Caravan[-idx.train, ]
# -------------------------- (b) --------------------------
ntree = 1000
lambda = 0.01
boost.fit = gbm(Purchase~., data=train, distribution='bernoulli', n.trees=ntree, shrinkage=lambda)
summary(boost.fit)
# The variable PPERSAUT seems the mose important
# -------------------------- (c) --------------------------
boost.pred = predict(boost.fit, newdata=test, n.trees=ntree, type='response')
boost.pred = ifelse(boost.pred>0.2, 1, 0)
table(pred=boost.pred, actual=test$Purchase)
# Fraction of predicted Yes is actual Yes (True Positive): 33/(33+123) = 0.2115385
# KNN: Cross-validating k omitted
k = 20
train.mat = subset(train, select=-Purchase)
test.mat = subset(test, select=-Purchase)
knn.pred = knn(train.mat, test.mat, train$Purchase, k=k, prob=TRUE)
knn.pred = attr(knn.pred, 'prob')
knn.pred = ifelse(knn.pred>0.2, 1, 0)
table(pred=knn.pred, actual=test$Purchase)
knn.pred = knn(train.mat, test.mat, train$Purchase, k=k, prob=TRUE)
knn.pred
knn.pred
attr(knn.pred)
idx = as.logical(knn.pred)
idx
idx = as.logical.factor(knn.pred)
idx
knn.pred[1:10]
knn.pred[1:100]
idx.0 = ifelse(knn.pred==0, TRUE, FALSE)
prob[idx.0] = 1 - prob[idx.0]
prob = attr(knn.pred, 'prob')
prob[idx.0] = 1 - prob[idx.0]
knn.pred = ifelse(prob>0.2, 1, 0)
table(pred=knn.pred, actual=test$Purchase)
library(class)
library(gbm)
library(ISLR)
attach(Caravan)
set.seed(1)
# -------------------------- (a) --------------------------
Caravan$Purchase = ifelse(Caravan$Purchase=='Yes', 1, 0)
idx.train = 1:1000
train = Caravan[idx.train, ]
test = Caravan[-idx.train, ]
# -------------------------- (b) --------------------------
ntree = 1000
lambda = 0.01
boost.fit = gbm(Purchase~., data=train, distribution='bernoulli', n.trees=ntree, shrinkage=lambda)
summary(boost.fit)
# The variable PPERSAUT seems the mose important
# -------------------------- (c) --------------------------
boost.pred = predict(boost.fit, newdata=test, n.trees=ntree, type='response')
boost.pred = ifelse(boost.pred>0.2, 1, 0)
table(pred=boost.pred, actual=test$Purchase)
# Fraction of predicted Yes is actual Yes (True Positive): 33/(33+123) = 0.2115385
# KNN: Cross-validating k omitted
k = 20
train.mat = subset(train, select=-Purchase)
test.mat = subset(test, select=-Purchase)
knn.pred = knn(train.mat, test.mat, train$Purchase, k=k, prob=TRUE)
prob = attr(knn.pred, 'prob')
idx.0 = ifelse(knn.pred==0, TRUE, FALSE)
prob[idx.0] = 1 - prob[idx.0]
knn.pred = ifelse(prob>0.2, 1, 0)
table(pred=knn.pred, actual=test$Purchase)
28/(138+28)
# logistic regression
log.fit = glm(Purchase~., data=train, family='binomial')
?glm
log.pred = predict(log.fit, newdata=test, type='response')
log.pred
log.pred = ifelse(log.pred>0.2, 1, 0)
table(pred=log.pred, actual=test$Purchase)
58/(350+58)
library(class)
library(gbm)
library(ISLR)
attach(Caravan)
set.seed(1)
# -------------------------- (a) --------------------------
Caravan$Purchase = ifelse(Caravan$Purchase=='Yes', 1, 0)
idx.train = 1:1000
train = Caravan[idx.train, ]
test = Caravan[-idx.train, ]
# -------------------------- (b) --------------------------
ntree = 1000
lambda = 0.01
boost.fit = gbm(Purchase~., data=train, distribution='bernoulli', n.trees=ntree, shrinkage=lambda)
summary(boost.fit)
# The variable PPERSAUT seems the mose important
# -------------------------- (c) --------------------------
boost.pred = predict(boost.fit, newdata=test, n.trees=ntree, type='response')
boost.pred = ifelse(boost.pred>0.2, 1, 0)
table(pred=boost.pred, actual=test$Purchase)
# Fraction of predicted Yes is actual Yes (True Positive): 33/(33+123) = 0.2115385
# KNN: Cross-validating k omitted
k = 20
train.mat = subset(train, select=-Purchase)
test.mat = subset(test, select=-Purchase)
knn.pred = knn(train.mat, test.mat, train$Purchase, k=k, prob=TRUE)
prob = attr(knn.pred, 'prob')
idx.0 = ifelse(knn.pred==0, TRUE, FALSE)
prob[idx.0] = 1 - prob[idx.0]
knn.pred = ifelse(prob>0.2, 1, 0)
table(pred=knn.pred, actual=test$Purchase)
# True Positive Rate: 28/(138+28) = 0.1686747
# logistic regression
log.fit = glm(Purchase~., data=train, family='binomial')
log.pred = predict(log.fit, newdata=test, type='response')
log.pred = ifelse(log.pred>0.2, 1, 0)
table(pred=log.pred, actual=test$Purchase)
# True Positive rate: 58/(350+58) = 0.1421569
# Conclusion: For the current choice of variables and parameters,
# and measured by true positive rate,
# it seems that the boosting method performs better than KNN and logistic regression
