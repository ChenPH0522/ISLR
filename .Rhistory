mat = table(pred=svm.pred, actual=ytest)
err.test[i] = (mat[1, 2]+mat[2, 1])/sum(mat)
}
plot(costs, err.test, type='b', xlab='cost', ylab='test error')
# cost = 5 leads to the smallest test error
# this is larger than
which.min(err.test)
costs[2]
library(e1071)
library(ISLR)
attach(Auto)
set.seed(1)
# --------------------------- (a) ---------------------------
med = median(mpg)
library(e1071)
library(ISLR)
attach(Auto)
set.seed(1)
# --------------------------- (a) ---------------------------
med = median(mpg)
mpgMed = ifelse(mpg>med, 1, 0)
# --------------------------- (b) ---------------------------
Auto = cbind(Auto, mpgMed)
costs = c(0.01, 0.1, 1, 5, 10, 15, 20, 30, 50)
svm.linear.cv = tune(svm, mpgMed~.-mpg, data=Auto, kernel='linear', ranges=list(cost=costs))
err.cv = svm.linear.cv$performance$error
plot(costs, err.cv, type='b', xlab='cost', ylab='cross-validation error')
which.min(err.cv)
costs[3]
# SVM: polynomial kernel
dgs = 1:5
svm.poly.cv = tune(svm, mpgMed~.-mpg, data=Auto, kernel='polynomial', ranges=list(cost=costs, degree=dgs))
summary(svm.poly.cv)
install.packages("plotly")
svm.poly.cv$performances
library(plotly)
x = svm.poly.cv$performances$cost
y = svm.poly.cv$performances$degree
y = svm.poly.cv$performances$error
y = svm.poly.cv$performances$degree
z = svm.poly.cv$performances$degree
z = svm.poly.cv$performances$error
plot_ly(x=x, y=y, z=z) %>% add_markers()
svm.poly.cv
summary(svm.poly.cv)
# SVM: radial kernel
gms = costs
svm.radial.cv = tune(svm, mpgMed~.-mpg, data=Auto, kernel='radial', ranges=list(cost=costs, gamma=gms))
summary(svm.radial.cv)
View(Auto)
plot(svm.radial.cv$best.model, Auto, names(Auto)[2:9])
plot(svm.radial.cv$best.model, Auto, names(Auto)[2:9])
plot(svm.radial.cv$best.model, Auto)
library(e1071)
library(ISLR)
attach(Auto)
set.seed(1)
# --------------------------- (a) ---------------------------
med = median(mpg)
mpgMed = ifelse(mpg>med, 1, 0)
# --------------------------- (b) ---------------------------
Auto = cbind(Auto, mpgMed)
costs = c(0.01, 0.1, 1, 5, 10, 15, 20, 30, 50)
svm.linear.cv = tune(svm, mpgMed~.-mpg, data=Auto, kernel='linear', ranges=list(cost=costs))
err.cv = svm.linear.cv$performance$error
plot(costs, err.cv, type='b', xlab='cost', ylab='cross-validation error')
# cost = 1 leads to the lowest cross-validation error
# --------------------------- (c) ---------------------------
# SVM: polynomial kernel
dgs = 1:5
svm.poly.cv = tune(svm, mpgMed~.-mpg, data=Auto, kernel='polynomial', ranges=list(cost=costs, degree=dgs))
# best parameters are: cost=50, degree=1
# best cross-validation error is: 0.09921599
# SVM: radial kernel
gms = costs
svm.radial.cv = tune(svm, mpgMed~.-mpg, data=Auto, kernel='radial', ranges=list(cost=costs, gamma=gms))
# best parameters are: cost=5, gamma=0.1
# best cross-validation error is: 0.06294489
class(svm.radial.cv$best.model)
svm.radial.cv$best.model
View(Auto)
plot(svm.radial.cv$best.model, Auto,, c('cylinders', 'displacement', 'horsepower'))
plot(svm.radial.cv$best.model, Auto, cylinders~horsepower)
plot(svm.radial.cv$best.model, Auto, 'cylinders'~'horsepower')
plot(svm.radial.cv$best.model, Auto, as.formula('mpgMed~cylinders+displacement))
)
)
')
''
plot(svm.radial.cv$best.model, Auto, as.formula('mpgMed~cylinders+displacement'))
plot(svm.radial.cv$best.model, Auto, as.formula('mpgMed~cylinders'))
fit = svm.radial.cv$best.model
plot(fit, Auto, as.formula('mpgMed~cylinders'))
fit = svm(mpgMed~., data=Auto, kernel='radial', cost=5, gamma=0.1)
plot(fit, Auto, as.formula('mpgMed~cylinders'))
View(fit)
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
idx.train = sample(nrow(OJ), 800)
train = OJ[idx.train, ]
test = OJ[-idx.train, ]
svm.linear = svm(Purchase~., data=train, kernel='linear', cost=0.01)
summary(svm.linear)
View(train)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# ------------------------ (c) ------------------------
mat.train = table(pred=svm.linear$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
svm.linear.pred = predict(svm.linear, newdata=test)
mat.test = table(pred=svm.linear.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# ------------------------ (d) ------------------------
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.linear.cv = tune(svm, Purchase~., data=train, kernel='linear', ranges=list(cost=costs))
summary(svm.linear.cv)
svm.linear.opt = svm.linear.cv$best.model
# ------------------------ (e) ------------------------
svm.linear.opt = svm.linear.cv$best.model
mat.opt.train = table(pred=svm.linear.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
svm.linear.opt.pred = predict(svm.linear.opt, newdata=test)
mat.opt.test = table(pred=svm.linear.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
# ------------------------ (a) ------------------------
idx.train = sample(nrow(OJ), 800)
train = OJ[idx.train, ]
test = OJ[-idx.train, ]
# ------------------------ (f) ------------------------
svm.radial = svm(Purchase~., data=train, kernel='radial', cost=0.01)
summary(svm.radial)
mat.train = table(pred=svm.radial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.175
svm.radial.pred = predict(svm.radial, newdata=test)
mat.test = table(pred=svm.radial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.177778
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.radial.cv = tune(svm, Purchase~., data=train, kernel='radial', ranges=list(cost=costs))
summary(svm.radial.cv)
# optimal cost=7
svm.radial.opt = svm.radial.cv$best.model
mat.opt.train = table(pred=svm.radial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1625
svm.radial.opt.pred = predict(svm.radial.opt, newdata=test)
mat.opt.test = table(pred=svm.radial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.1519
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
# ------------------------ (a) ------------------------
idx.train = sample(nrow(OJ), 800)
train = OJ[idx.train, ]
test = OJ[-idx.train, ]
# ------------------------ (g) ------------------------
svm.polynomial = svm(Purchase~., data=train, kernel='polynomial', cost=0.01)
summary(svm.polynomial)
mat.train = table(pred=svm.polynomial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.39375
svm.polynomial.pred = predict(svm.polynomial, newdata=test)
mat.test = table(pred=svm.polynomial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.37778
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.polynomial.cv = tune(svm, Purchase~., data=train, kernel='polynomial', ranges=list(cost=costs))
summary(svm.polynomial.cv)
# optimal cost=0.5
svm.polynomial.opt = svm.polynomial.cv$best.model
mat.opt.train = table(pred=svm.polynomial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1425
svm.polynomial.opt.pred = predict(svm.polynomial.opt, newdata=test)
mat.opt.test = table(pred=svm.polynomial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.17778
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
# ------------------------ (g) ------------------------
svm.polynomial = svm(Purchase~., data=train, kernel='polynomial', cost=0.01, degree=2)
summary(svm.polynomial)
mat.train = table(pred=svm.polynomial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.37125
svm.polynomial.pred = predict(svm.polynomial, newdata=test)
mat.test = table(pred=svm.polynomial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.3630
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.polynomial.cv = tune(svm, Purchase~., data=train, kernel='polynomial', degree=2, ranges=list(cost=costs))
summary(svm.polynomial.cv)
# optimal cost=0.5
svm.polynomial.opt = svm.polynomial.cv$best.model
mat.opt.train = table(pred=svm.polynomial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1425
svm.polynomial.opt.pred = predict(svm.polynomial.opt, newdata=test)
mat.opt.test = table(pred=svm.polynomial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.17778
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
# ------------------------ (a) ------------------------
idx.train = sample(nrow(OJ), 800)
train = OJ[idx.train, ]
test = OJ[-idx.train, ]
svm.polynomial = svm(Purchase~., data=train, kernel='polynomial', cost=0.01, degree=2)
summary(svm.polynomial)
mat.train = table(pred=svm.polynomial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.37125
svm.polynomial.pred = predict(svm.polynomial, newdata=test)
mat.test = table(pred=svm.polynomial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.3630
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.polynomial.cv = tune(svm, Purchase~., data=train, kernel='polynomial', degree=2, ranges=list(cost=costs))
summary(svm.polynomial.cv)
# optimal cost=0.5
svm.polynomial.opt = svm.polynomial.cv$best.model
mat.opt.train = table(pred=svm.polynomial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1425
svm.polynomial.opt.pred = predict(svm.polynomial.opt, newdata=test)
mat.opt.test = table(pred=svm.polynomial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.17778
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
# ------------------------ (a) ------------------------
idx.train = sample(nrow(OJ), 800)
train = OJ[idx.train, ]
test = OJ[-idx.train, ]
# ------------------------ (b) ------------------------
svm.linear = svm(Purchase~., data=train, kernel='linear', cost=0.01)
summary(svm.linear)
# ------------------------ (c) ------------------------
mat.train = table(pred=svm.linear$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.175
svm.linear.pred = predict(svm.linear, newdata=test)
mat.test = table(pred=svm.linear.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.177778
# ------------------------ (d) ------------------------
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.linear.cv = tune(svm, Purchase~., data=train, kernel='linear', ranges=list(cost=costs))
summary(svm.linear.cv)
# optimal cost=7
# ------------------------ (e) ------------------------
svm.linear.opt = svm.linear.cv$best.model
mat.opt.train = table(pred=svm.linear.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1625
svm.linear.opt.pred = predict(svm.linear.opt, newdata=test)
mat.opt.test = table(pred=svm.linear.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.1519
# ------------------------ (f) ------------------------
svm.radial = svm(Purchase~., data=train, kernel='radial', cost=0.01)
summary(svm.radial)
mat.train = table(pred=svm.radial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.39375
svm.radial.pred = predict(svm.radial, newdata=test)
mat.test = table(pred=svm.radial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.37778
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.radial.cv = tune(svm, Purchase~., data=train, kernel='radial', ranges=list(cost=costs))
summary(svm.radial.cv)
# optimal cost=0.5
svm.radial.opt = svm.radial.cv$best.model
mat.opt.train = table(pred=svm.radial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1425
svm.radial.opt.pred = predict(svm.radial.opt, newdata=test)
mat.opt.test = table(pred=svm.radial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.17778
# ------------------------ (g) ------------------------
svm.polynomial = svm(Purchase~., data=train, kernel='polynomial', cost=0.01, degree=2)
summary(svm.polynomial)
mat.train = table(pred=svm.polynomial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.37125
svm.polynomial.pred = predict(svm.polynomial, newdata=test)
mat.test = table(pred=svm.polynomial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.36667
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.polynomial.cv = tune(svm, Purchase~., data=train, kernel='polynomial', degree=2, ranges=list(cost=costs))
summary(svm.polynomial.cv)
# optimal cost=3
svm.polynomial.opt = svm.polynomial.cv$best.model
mat.opt.train = table(pred=svm.polynomial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.15375
svm.polynomial.opt.pred = predict(svm.polynomial.opt, newdata=test)
mat.opt.test = table(pred=svm.polynomial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.2037
rownames(USArrests)
colnames(USArrests)
?row.names
states = rownames(USArrests)
vars = colnames(USArrests)
apply(USArrests, 2, mean)
apply(USArrests, 2, var)
pr.out = prcomp(USArrests, scale=TRUE)
names(pr.out)
pr.out$center
pr.out$scale
(pr.out$scale)^2
pr.out$rotation
pr.out$x
dim(pr.out$x)
biplot(pr.out$x, scale=0)
biplot(pr.out, scale=0)
?biplot
biplot(pr.out, scale=1)
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale=0)
pr.out$sdev
pr.var = (pr.out$sdev)^2
pr.var
pve = pr.var/sum(pr.var)
pve
plot(pve, type='b', ylim=c(0, 1), xlab='Principal Component', ylab='Proportion of Variance Explained')
plot(cumsum(pve), type='b', ylim=c(0, 1), xlab='Principal Component', ylab='Cumulative Proportion of Variance Explained')
# ------------------- 10.5.1 -------------------
set.seed(1)
x = matrix(rnorm(50*2), ncol=2)
x[1:25, 1] = x[1:25, 1] + 3
x[1:25, 2] = x[1:25, 2] - 4
?kmeans
km.out = kmeans(x, centers=2, nstart=20)
km.out$cluster
plot(x, col=km.out$cluster, xlab='x1', ylab='x2', pch=20, cex=2)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(4)
km.out = kmeans(x, centers=3, nstart=20)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(3)
km.out = kmeans(x, centers=3, nstart=1)
km.out$tot.withinss
km.out = kmeans(x, centers=3, nstart=20)
km.out$tot.withinss
set.seed(3)
km.out = kmeans(x, centers=3, nstart=1)
km.out$tot.withinss
km.out = kmeans(x, centers=3, nstart=20)
km.out$tot.withinss
# ------------------- 10.5.1 -------------------
set.seed(1)
x = matrix(rnorm(50*2), ncol=2)
x[1:25, 1] = x[1:25, 1] + 3
x[1:25, 2] = x[1:25, 2] - 4
km.out = kmeans(x, centers=2, nstart=20)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(4)
km.out = kmeans(x, centers=3, nstart=20)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(3)
km.out = kmeans(x, centers=3, nstart=1)
km.out$tot.withinss
km.out = kmeans(x, centers=3, nstart=20)
km.out$tot.withinss
km.out$withinss
# ------------------- 10.5.2 -------------------
hc.complete = hclust(dist(x), method='complete')
hc.complete
?dist
hc.average = hclust(dist(x), method='average')
hc.single = hclust(dist(x), method='single')
par(mfrow=c(1, 3))
plot(hc.complete, main='Complete Linkage', xlab='', ylab='', cex=0.9)
plot(hc.average, main='Average Linkage', xlab='', ylab='', cex=0.9)
plot(hc.single, main='Single Linkage', cex=0.9)
par(mfrow=c(1, 3))
plot(hc.complete, main='Complete Linkage', xlab='', ylab='', cex=0.9)
plot(hc.average, main='Average Linkage', xlab='', ylab='', cex=0.9)
plot(hc.single, main='Single Linkage', xlab='', ylab='', cex=0.9)
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
xsc = scale(x)
hc.scale.complete = hclust(dist(xsc), method='complete')
plot(mfrow=c(1, 1))
plot(hc.scale, main='Scaled Complete Linkage', xlab='', ylab='', cex=0.9)
par(mfrow=c(1, 1))
plot(hc.scale, main='Scaled Complete Linkage', xlab='', ylab='', cex=0.9)
plot(hc.scale.complete, main='Scaled Complete Linkage', xlab='', ylab='', cex=0.9)
?cor
x = matrix(rnorm(30*3), ncol=3)
dd = as.dist(1 - cor(t(x)))
hc.dd.complete = hclust(dd, method='complete')
x = matrix(rnorm(30*3), ncol=3)
dd = as.dist(1 - cor(t(x)))
hc.dd.complete = hclust(dd, method='complete')
plot(hc.dd.complete, main='Correlation Distance - Complete Linkage', xlab='', ylab='', cex=0.9)
par(mar=c(1,1,1,1))
# ------------------- 10.5.1 -------------------
set.seed(1)
x = matrix(rnorm(50*2), ncol=2)
x[1:25, 1] = x[1:25, 1] + 3
x[1:25, 2] = x[1:25, 2] - 4
km.out = kmeans(x, centers=2, nstart=20)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(4)
km.out = kmeans(x, centers=3, nstart=20)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(3)
km.out = kmeans(x, centers=3, nstart=1)
km.out$tot.withinss
# [1] 60.17588
km.out = kmeans(x, centers=3, nstart=20)
km.out$tot.withinss
# [1] 60.17588
# ------------------- 10.5.2 -------------------
hc.complete = hclust(dist(x), method='complete')
hc.average = hclust(dist(x), method='average')
hc.single = hclust(dist(x), method='single')
par(mfrow=c(1, 3))
plot(hc.complete, main='Complete Linkage', xlab='', ylab='', cex=0.9)
plot(hc.average, main='Average Linkage', xlab='', ylab='', cex=0.9)
plot(hc.single, main='Single Linkage', xlab='', ylab='', cex=0.9)
xsc = scale(x)
hc.scale.complete = hclust(dist(xsc), method='complete')
par(mfrow=c(1, 1))
plot(hc.scale.complete, main='Scaled Complete Linkage', xlab='', ylab='', cex=0.9)
x = matrix(rnorm(30*3), ncol=3)
dd = as.dist(1 - cor(t(x)))
hc.dd.complete = hclust(dd, method='complete')
plot(hc.dd.complete, main='Correlation Distance - Complete Linkage', xlab='', ylab='', cex=0.9)
library(ISLR)
nci.labs = NCI60$labs
nci.data = NCI60$data
View(nci.data)
dim(nci.data)
dim(nci.data)
table(nci.labs)
# ------------------------------ 10.6.1 ------------------------------
pr.out = prcomp(nci.data, scale=TRUE)
plot(pr.out$x[, 1:2], col=cols(nci.labs), xlab='Z1', ylab='Z2', pch=19)
cols <- function(vec){
cols = rainbow(length(unique(vec)))
return( cols[as.numeric(as.factor(vec))] )
}
plot(pr.out$x[, 1:2], col=cols(nci.labs), xlab='Z1', ylab='Z2', pch=19)
plot(pr.out$x[, c(1, 3)], col=cols(nci.labs), xlab='Z1', ylab='Z2', pch=19)
View(data[1:2, 1:4])
View(nci.data[1:2, 1:4])
summary(pr.out)
plot(pr.out)
pve = (pr.out$sdev)^2 / sum((pr.out$sdev)^2)
pve = (pr.out$sdev)^2 / sum((pr.out$sdev)^2)
par(mfrow=c(1, 2))
plot(pve, ylim=c(0, 1), type='b', xlab='Principal Component', ylab='Proportion of Variance Explained')
plot(cumsum(pve), ylim=c(0, 1), type='b', xlab='Principal Component', ylabo='Cumulative Proportion of Variance Explained')
pve = (pr.out$sdev)^2 / sum((pr.out$sdev)^2)
par(mfrow=c(1, 2))
plot(pve, ylim=c(0, 1), type='b', xlab='Principal Component', ylab='Proportion of Variance Explained')
plot(cumsum(pve), ylim=c(0, 1), type='b', xlab='Principal Component', ylab='Cumulative Proportion of Variance Explained')
summary(pr.out)
# ------------------------------ 10.6.2 ------------------------------
sd.data = scale(nci.data)
# ------------------------------ 10.6.2 ------------------------------
sd.data = scale(nci.data)
hc.complete = hclust(dist(sd.data), method='complete')
hc.single = hclust(dist(sd.data), method='single')
hc.average = hclust(dist(sd.data), method='average')
par(mfrow=c(1, 3))
plot(hc.complete, labels=nci.labs, xlab='', ylab='', main='Complete Linkage')
plot(hc.single, labels=nci.labs, xlab='', ylab='', main='Single Linkage')
plot(hc.average, labels=nci.labs, xlab='', ylab='', main='Average Linkage')
hc.complete.cut = cutree(hc.complete, 4)
table(hc.complete.cut, nci.labs)
par(mfrow=c(1, 1))
plot(hc.complete, labels=nci.labs, xlab='', ylab='', main='Complete Linkage')
abline(h=139, col='red')
hc.complete
?kmeans
set.seed(1)
km.out = kmeans(sd.data, centers=4, nstart=20)
table(km.cluster, nci.labs)
km.cluster = km.out$cluster
table(km.cluster, nci.labs)
table(km.cluster, hc.complete.cut)
set.seed(2)
km.out = kmeans(sd.data, centers=4, nstart=20)
km.cluster = km.out$cluster
table(km.cluster, hc.complete.cut)
hc.score.cp = hclust(dist(pr.out$x[, 1:5]), method='complete')
plot(hc.score.cp, labels=nci.labs, xlab='', ylab='', main='Principal Component Score - Complete Linkage')
table(cutree(hc.score.cp, 4), nci.labs)
