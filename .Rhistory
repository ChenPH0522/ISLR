class(svm.radial.cv$best.model)
svm.radial.cv$best.model
View(Auto)
plot(svm.radial.cv$best.model, Auto,, c('cylinders', 'displacement', 'horsepower'))
plot(svm.radial.cv$best.model, Auto, cylinders~horsepower)
plot(svm.radial.cv$best.model, Auto, 'cylinders'~'horsepower')
plot(svm.radial.cv$best.model, Auto, as.formula('mpgMed~cylinders+displacement))
)
)
')
''
plot(svm.radial.cv$best.model, Auto, as.formula('mpgMed~cylinders+displacement'))
plot(svm.radial.cv$best.model, Auto, as.formula('mpgMed~cylinders'))
fit = svm.radial.cv$best.model
plot(fit, Auto, as.formula('mpgMed~cylinders'))
fit = svm(mpgMed~., data=Auto, kernel='radial', cost=5, gamma=0.1)
plot(fit, Auto, as.formula('mpgMed~cylinders'))
View(fit)
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
idx.train = sample(nrow(OJ), 800)
train = OJ[idx.train, ]
test = OJ[-idx.train, ]
svm.linear = svm(Purchase~., data=train, kernel='linear', cost=0.01)
summary(svm.linear)
View(train)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# ------------------------ (c) ------------------------
mat.train = table(pred=svm.linear$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
svm.linear.pred = predict(svm.linear, newdata=test)
mat.test = table(pred=svm.linear.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# ------------------------ (d) ------------------------
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.linear.cv = tune(svm, Purchase~., data=train, kernel='linear', ranges=list(cost=costs))
summary(svm.linear.cv)
svm.linear.opt = svm.linear.cv$best.model
# ------------------------ (e) ------------------------
svm.linear.opt = svm.linear.cv$best.model
mat.opt.train = table(pred=svm.linear.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
svm.linear.opt.pred = predict(svm.linear.opt, newdata=test)
mat.opt.test = table(pred=svm.linear.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
# ------------------------ (a) ------------------------
idx.train = sample(nrow(OJ), 800)
train = OJ[idx.train, ]
test = OJ[-idx.train, ]
# ------------------------ (f) ------------------------
svm.radial = svm(Purchase~., data=train, kernel='radial', cost=0.01)
summary(svm.radial)
mat.train = table(pred=svm.radial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.175
svm.radial.pred = predict(svm.radial, newdata=test)
mat.test = table(pred=svm.radial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.177778
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.radial.cv = tune(svm, Purchase~., data=train, kernel='radial', ranges=list(cost=costs))
summary(svm.radial.cv)
# optimal cost=7
svm.radial.opt = svm.radial.cv$best.model
mat.opt.train = table(pred=svm.radial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1625
svm.radial.opt.pred = predict(svm.radial.opt, newdata=test)
mat.opt.test = table(pred=svm.radial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.1519
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
# ------------------------ (a) ------------------------
idx.train = sample(nrow(OJ), 800)
train = OJ[idx.train, ]
test = OJ[-idx.train, ]
# ------------------------ (g) ------------------------
svm.polynomial = svm(Purchase~., data=train, kernel='polynomial', cost=0.01)
summary(svm.polynomial)
mat.train = table(pred=svm.polynomial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.39375
svm.polynomial.pred = predict(svm.polynomial, newdata=test)
mat.test = table(pred=svm.polynomial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.37778
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.polynomial.cv = tune(svm, Purchase~., data=train, kernel='polynomial', ranges=list(cost=costs))
summary(svm.polynomial.cv)
# optimal cost=0.5
svm.polynomial.opt = svm.polynomial.cv$best.model
mat.opt.train = table(pred=svm.polynomial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1425
svm.polynomial.opt.pred = predict(svm.polynomial.opt, newdata=test)
mat.opt.test = table(pred=svm.polynomial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.17778
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
# ------------------------ (g) ------------------------
svm.polynomial = svm(Purchase~., data=train, kernel='polynomial', cost=0.01, degree=2)
summary(svm.polynomial)
mat.train = table(pred=svm.polynomial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.37125
svm.polynomial.pred = predict(svm.polynomial, newdata=test)
mat.test = table(pred=svm.polynomial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.3630
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.polynomial.cv = tune(svm, Purchase~., data=train, kernel='polynomial', degree=2, ranges=list(cost=costs))
summary(svm.polynomial.cv)
# optimal cost=0.5
svm.polynomial.opt = svm.polynomial.cv$best.model
mat.opt.train = table(pred=svm.polynomial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1425
svm.polynomial.opt.pred = predict(svm.polynomial.opt, newdata=test)
mat.opt.test = table(pred=svm.polynomial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.17778
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
# ------------------------ (a) ------------------------
idx.train = sample(nrow(OJ), 800)
train = OJ[idx.train, ]
test = OJ[-idx.train, ]
svm.polynomial = svm(Purchase~., data=train, kernel='polynomial', cost=0.01, degree=2)
summary(svm.polynomial)
mat.train = table(pred=svm.polynomial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.37125
svm.polynomial.pred = predict(svm.polynomial, newdata=test)
mat.test = table(pred=svm.polynomial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.3630
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.polynomial.cv = tune(svm, Purchase~., data=train, kernel='polynomial', degree=2, ranges=list(cost=costs))
summary(svm.polynomial.cv)
# optimal cost=0.5
svm.polynomial.opt = svm.polynomial.cv$best.model
mat.opt.train = table(pred=svm.polynomial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1425
svm.polynomial.opt.pred = predict(svm.polynomial.opt, newdata=test)
mat.opt.test = table(pred=svm.polynomial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.17778
library(e1071)
library(ISLR)
attach(OJ)
set.seed(1)
# ------------------------ (a) ------------------------
idx.train = sample(nrow(OJ), 800)
train = OJ[idx.train, ]
test = OJ[-idx.train, ]
# ------------------------ (b) ------------------------
svm.linear = svm(Purchase~., data=train, kernel='linear', cost=0.01)
summary(svm.linear)
# ------------------------ (c) ------------------------
mat.train = table(pred=svm.linear$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.175
svm.linear.pred = predict(svm.linear, newdata=test)
mat.test = table(pred=svm.linear.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.177778
# ------------------------ (d) ------------------------
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.linear.cv = tune(svm, Purchase~., data=train, kernel='linear', ranges=list(cost=costs))
summary(svm.linear.cv)
# optimal cost=7
# ------------------------ (e) ------------------------
svm.linear.opt = svm.linear.cv$best.model
mat.opt.train = table(pred=svm.linear.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1625
svm.linear.opt.pred = predict(svm.linear.opt, newdata=test)
mat.opt.test = table(pred=svm.linear.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.1519
# ------------------------ (f) ------------------------
svm.radial = svm(Purchase~., data=train, kernel='radial', cost=0.01)
summary(svm.radial)
mat.train = table(pred=svm.radial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.39375
svm.radial.pred = predict(svm.radial, newdata=test)
mat.test = table(pred=svm.radial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.37778
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.radial.cv = tune(svm, Purchase~., data=train, kernel='radial', ranges=list(cost=costs))
summary(svm.radial.cv)
# optimal cost=0.5
svm.radial.opt = svm.radial.cv$best.model
mat.opt.train = table(pred=svm.radial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.1425
svm.radial.opt.pred = predict(svm.radial.opt, newdata=test)
mat.opt.test = table(pred=svm.radial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.17778
# ------------------------ (g) ------------------------
svm.polynomial = svm(Purchase~., data=train, kernel='polynomial', cost=0.01, degree=2)
summary(svm.polynomial)
mat.train = table(pred=svm.polynomial$fitted, actual=train$Purchase)
err.train = (mat.train[1, 2] + mat.train[2, 1])/sum(mat.train)
# training error = 0.37125
svm.polynomial.pred = predict(svm.polynomial, newdata=test)
mat.test = table(pred=svm.polynomial.pred, actual=test$Purchase)
err.test = (mat.test[1, 2] + mat.test[2, 1])/sum(mat.test)
# test error = 0.36667
costs = c(0.01, 0.1, 0.5, 1, 3, 5, 7, 10)
svm.polynomial.cv = tune(svm, Purchase~., data=train, kernel='polynomial', degree=2, ranges=list(cost=costs))
summary(svm.polynomial.cv)
# optimal cost=3
svm.polynomial.opt = svm.polynomial.cv$best.model
mat.opt.train = table(pred=svm.polynomial.opt$fitted, actual=train$Purchase)
err.opt.train = (mat.opt.train[1,2]+mat.opt.train[2,1])/sum(mat.opt.train)
# training error = 0.15375
svm.polynomial.opt.pred = predict(svm.polynomial.opt, newdata=test)
mat.opt.test = table(pred=svm.polynomial.opt.pred, actual=test$Purchase)
err.opt.test = ( mat.opt.test[1,2]+mat.opt.test[2,1] )/sum(mat.opt.test)
# test error = 0.2037
rownames(USArrests)
colnames(USArrests)
?row.names
states = rownames(USArrests)
vars = colnames(USArrests)
apply(USArrests, 2, mean)
apply(USArrests, 2, var)
pr.out = prcomp(USArrests, scale=TRUE)
names(pr.out)
pr.out$center
pr.out$scale
(pr.out$scale)^2
pr.out$rotation
pr.out$x
dim(pr.out$x)
biplot(pr.out$x, scale=0)
biplot(pr.out, scale=0)
?biplot
biplot(pr.out, scale=1)
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale=0)
pr.out$sdev
pr.var = (pr.out$sdev)^2
pr.var
pve = pr.var/sum(pr.var)
pve
plot(pve, type='b', ylim=c(0, 1), xlab='Principal Component', ylab='Proportion of Variance Explained')
plot(cumsum(pve), type='b', ylim=c(0, 1), xlab='Principal Component', ylab='Cumulative Proportion of Variance Explained')
# ------------------- 10.5.1 -------------------
set.seed(1)
x = matrix(rnorm(50*2), ncol=2)
x[1:25, 1] = x[1:25, 1] + 3
x[1:25, 2] = x[1:25, 2] - 4
?kmeans
km.out = kmeans(x, centers=2, nstart=20)
km.out$cluster
plot(x, col=km.out$cluster, xlab='x1', ylab='x2', pch=20, cex=2)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(4)
km.out = kmeans(x, centers=3, nstart=20)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(3)
km.out = kmeans(x, centers=3, nstart=1)
km.out$tot.withinss
km.out = kmeans(x, centers=3, nstart=20)
km.out$tot.withinss
set.seed(3)
km.out = kmeans(x, centers=3, nstart=1)
km.out$tot.withinss
km.out = kmeans(x, centers=3, nstart=20)
km.out$tot.withinss
# ------------------- 10.5.1 -------------------
set.seed(1)
x = matrix(rnorm(50*2), ncol=2)
x[1:25, 1] = x[1:25, 1] + 3
x[1:25, 2] = x[1:25, 2] - 4
km.out = kmeans(x, centers=2, nstart=20)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(4)
km.out = kmeans(x, centers=3, nstart=20)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(3)
km.out = kmeans(x, centers=3, nstart=1)
km.out$tot.withinss
km.out = kmeans(x, centers=3, nstart=20)
km.out$tot.withinss
km.out$withinss
# ------------------- 10.5.2 -------------------
hc.complete = hclust(dist(x), method='complete')
hc.complete
?dist
hc.average = hclust(dist(x), method='average')
hc.single = hclust(dist(x), method='single')
par(mfrow=c(1, 3))
plot(hc.complete, main='Complete Linkage', xlab='', ylab='', cex=0.9)
plot(hc.average, main='Average Linkage', xlab='', ylab='', cex=0.9)
plot(hc.single, main='Single Linkage', cex=0.9)
par(mfrow=c(1, 3))
plot(hc.complete, main='Complete Linkage', xlab='', ylab='', cex=0.9)
plot(hc.average, main='Average Linkage', xlab='', ylab='', cex=0.9)
plot(hc.single, main='Single Linkage', xlab='', ylab='', cex=0.9)
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
xsc = scale(x)
hc.scale.complete = hclust(dist(xsc), method='complete')
plot(mfrow=c(1, 1))
plot(hc.scale, main='Scaled Complete Linkage', xlab='', ylab='', cex=0.9)
par(mfrow=c(1, 1))
plot(hc.scale, main='Scaled Complete Linkage', xlab='', ylab='', cex=0.9)
plot(hc.scale.complete, main='Scaled Complete Linkage', xlab='', ylab='', cex=0.9)
?cor
x = matrix(rnorm(30*3), ncol=3)
dd = as.dist(1 - cor(t(x)))
hc.dd.complete = hclust(dd, method='complete')
x = matrix(rnorm(30*3), ncol=3)
dd = as.dist(1 - cor(t(x)))
hc.dd.complete = hclust(dd, method='complete')
plot(hc.dd.complete, main='Correlation Distance - Complete Linkage', xlab='', ylab='', cex=0.9)
par(mar=c(1,1,1,1))
# ------------------- 10.5.1 -------------------
set.seed(1)
x = matrix(rnorm(50*2), ncol=2)
x[1:25, 1] = x[1:25, 1] + 3
x[1:25, 2] = x[1:25, 2] - 4
km.out = kmeans(x, centers=2, nstart=20)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(4)
km.out = kmeans(x, centers=3, nstart=20)
plot(x, col=km.out$cluster+1, xlab='x1', ylab='x2', pch=20, cex=2)
set.seed(3)
km.out = kmeans(x, centers=3, nstart=1)
km.out$tot.withinss
# [1] 60.17588
km.out = kmeans(x, centers=3, nstart=20)
km.out$tot.withinss
# [1] 60.17588
# ------------------- 10.5.2 -------------------
hc.complete = hclust(dist(x), method='complete')
hc.average = hclust(dist(x), method='average')
hc.single = hclust(dist(x), method='single')
par(mfrow=c(1, 3))
plot(hc.complete, main='Complete Linkage', xlab='', ylab='', cex=0.9)
plot(hc.average, main='Average Linkage', xlab='', ylab='', cex=0.9)
plot(hc.single, main='Single Linkage', xlab='', ylab='', cex=0.9)
xsc = scale(x)
hc.scale.complete = hclust(dist(xsc), method='complete')
par(mfrow=c(1, 1))
plot(hc.scale.complete, main='Scaled Complete Linkage', xlab='', ylab='', cex=0.9)
x = matrix(rnorm(30*3), ncol=3)
dd = as.dist(1 - cor(t(x)))
hc.dd.complete = hclust(dd, method='complete')
plot(hc.dd.complete, main='Correlation Distance - Complete Linkage', xlab='', ylab='', cex=0.9)
library(ISLR)
nci.labs = NCI60$labs
nci.data = NCI60$data
View(nci.data)
dim(nci.data)
dim(nci.data)
table(nci.labs)
# ------------------------------ 10.6.1 ------------------------------
pr.out = prcomp(nci.data, scale=TRUE)
plot(pr.out$x[, 1:2], col=cols(nci.labs), xlab='Z1', ylab='Z2', pch=19)
cols <- function(vec){
cols = rainbow(length(unique(vec)))
return( cols[as.numeric(as.factor(vec))] )
}
plot(pr.out$x[, 1:2], col=cols(nci.labs), xlab='Z1', ylab='Z2', pch=19)
plot(pr.out$x[, c(1, 3)], col=cols(nci.labs), xlab='Z1', ylab='Z2', pch=19)
View(data[1:2, 1:4])
View(nci.data[1:2, 1:4])
summary(pr.out)
plot(pr.out)
pve = (pr.out$sdev)^2 / sum((pr.out$sdev)^2)
pve = (pr.out$sdev)^2 / sum((pr.out$sdev)^2)
par(mfrow=c(1, 2))
plot(pve, ylim=c(0, 1), type='b', xlab='Principal Component', ylab='Proportion of Variance Explained')
plot(cumsum(pve), ylim=c(0, 1), type='b', xlab='Principal Component', ylabo='Cumulative Proportion of Variance Explained')
pve = (pr.out$sdev)^2 / sum((pr.out$sdev)^2)
par(mfrow=c(1, 2))
plot(pve, ylim=c(0, 1), type='b', xlab='Principal Component', ylab='Proportion of Variance Explained')
plot(cumsum(pve), ylim=c(0, 1), type='b', xlab='Principal Component', ylab='Cumulative Proportion of Variance Explained')
summary(pr.out)
# ------------------------------ 10.6.2 ------------------------------
sd.data = scale(nci.data)
# ------------------------------ 10.6.2 ------------------------------
sd.data = scale(nci.data)
hc.complete = hclust(dist(sd.data), method='complete')
hc.single = hclust(dist(sd.data), method='single')
hc.average = hclust(dist(sd.data), method='average')
par(mfrow=c(1, 3))
plot(hc.complete, labels=nci.labs, xlab='', ylab='', main='Complete Linkage')
plot(hc.single, labels=nci.labs, xlab='', ylab='', main='Single Linkage')
plot(hc.average, labels=nci.labs, xlab='', ylab='', main='Average Linkage')
hc.complete.cut = cutree(hc.complete, 4)
table(hc.complete.cut, nci.labs)
par(mfrow=c(1, 1))
plot(hc.complete, labels=nci.labs, xlab='', ylab='', main='Complete Linkage')
abline(h=139, col='red')
hc.complete
?kmeans
set.seed(1)
km.out = kmeans(sd.data, centers=4, nstart=20)
table(km.cluster, nci.labs)
km.cluster = km.out$cluster
table(km.cluster, nci.labs)
table(km.cluster, hc.complete.cut)
set.seed(2)
km.out = kmeans(sd.data, centers=4, nstart=20)
km.cluster = km.out$cluster
table(km.cluster, hc.complete.cut)
hc.score.cp = hclust(dist(pr.out$x[, 1:5]), method='complete')
plot(hc.score.cp, labels=nci.labs, xlab='', ylab='', main='Principal Component Score - Complete Linkage')
table(cutree(hc.score.cp, 4), nci.labs)
states = rownames(USArrests)
vars = colnames(USArrests)
apply(USArrests, 2, mean)
# Murder  Assault UrbanPop     Rape
# 7.788  170.760   65.540   21.232
apply(USArrests, 2, var)
# Murder    Assault   UrbanPop       Rape
# 18.97047 6945.16571  209.51878   87.72916
pr.out = prcomp(USArrests, scale=TRUE)
names(pr.out)
# "sdev"     "rotation" "center"   "scale"    "x"
pr.out$center
# Murder  Assault UrbanPop     Rape
# 7.788  170.760   65.540   21.232
(pr.out$scale)^2
# Murder    Assault   UrbanPop       Rape
# 18.97047 6945.16571  209.51878   87.72916
pr.out$rotation
#           PC1        PC2        PC3         PC4
# Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
# Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
# UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
# Rape     -0.5434321 -0.1673186  0.8177779  0.08902432
dim(pr.out$x)
# [1] 50  4
biplot(pr.out, scale=0)
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale=0)
pr.out$sdev
# [1] 1.5748783 0.9948694 0.5971291 0.4164494
pr.var = (pr.out$sdev)^2
pr.var
# [1] 2.4802416 0.9897652 0.3565632 0.1734301
pve = pr.var/sum(pr.var)
pve
# [1] 0.62006039 0.24744129 0.08914080 0.04335752
plot(pve, type='b', ylim=c(0, 1), xlab='Principal Component', ylab='Proportion of Variance Explained')
plot(cumsum(pve), type='b', ylim=c(0, 1), xlab='Principal Component', ylab='Cumulative Proportion of Variance Explained')
x = scale(USArrests)
View(x)
View(x)
apply(x, 2, var)
set.seed(1)
control = matrix(rnorm(1000*50), ncol=50)
treatment = matrix(rnorm(1000*50), ncol=50)
x = cbind(control, treatment)
View(x)
View(x[1:10, ])
x[1, ] = seq(-18, 18-0.36, 0.36)
View(x[1:10, ])
pr.out = prcomp(scale(x))
a = summary(pr.out)
a$importance
summary(pr.out)$importance[, 1]
summary(pr.out)$importance[, 1]
new.feature = c(rep(1, 50), rep(0, 50))
x = rbind(x, new.feature)
pr.out = prcomp(scale(x))
summary(pr.out)$importance[, 1]
new.feature = c(rep(10, 50), rep(0, 50))
x = rbind(x, new.feature)
pr.out = prcomp(scale(x))
summary(pr.out)$importance[, 1]
set.seed(1)
control = matrix(rnorm(1000*50), ncol=50)
treatment = matrix(rnorm(1000*50), ncol=50)
x = cbind(control, treatment)
x[1, ] = seq(-18, 18-0.36, 0.36)
# suppose one gene is activated/de-activated by one type of machine
# and also the gene is related to time of process
pr.out = prcomp(scale(x))
summary(pr.out)$importance[, 1]
# the first principle component explains 9.911% of total variance
new.feature = c(rep(10, 50), rep(0, 50))
x = rbind(x, new.feature)
pr.out = prcomp(scale(x))
summary(pr.out)$importance[, 1]
