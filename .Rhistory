}
i = 3
coefi = coefficients(bestsub.model, id=i)
coefi
class(coefi)
x_cols = names(coefi)[-1]
x_cols
colnames(df.train)
df.train[, x_cols][1:10, ]
Y.pred = df.train[, x_cols] %*% coefi
class(df.train[, x_cols])
matrix( df.train[, x_cols] ) %*% coefi
A = df.train[, x_cols]
A[1:10, ]
A %*% c(1, 0, 0)
A = matrix( df.train[, x_cols] )
A[1:10, ]
A
View(A)
View(A)
A = data.matrix( df.train[, x_cols] )
for (i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)[-1]
Y.pred = data.matrix( df.train[, x_cols] ) %*% coefi
mse[i] = mean( (Y.test - Y.pred)^2 )
}
# ------------------ Exercise 10 ------------------
n <- 1000
p <- 20
# (a)
set.seed(1)
X = poly(rnorm(n), p, raw=T)
beta = rnorm(p) * sample(0:1, p, replace=T)
eps = rnorm(n)
Y = X %*% beta + eps
# (b)
n.train <- 100
train = sample(n, n.train, replace=F)
test = -(train)
X.train = X[train, ]
X.test = X[test, ]
Y.train = Y[train]
Y.test = Y[test]
df.train = data.frame(y=Y.train, x=X.train)
df.test = data.frame(y=Y.test, x=X.test)
df.full = data.frame(y=Y, x=X)
# (c)
library(leaps)
bestsub.model = regsubsets(y~., data=df.train, nvmax=p)
mse.train = rep(NA, p)
colnames(df.train) = colnames(df.train, prefix='x.')
for (i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)[-1]
Y.pred = data.matrix( df.test[, x_cols] ) %*% coefi
mse[i] = mean( (Y.train - Y.pred)^2 )
}
for (i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)[-1]
Y.pred = data.matrix( df.test[, x_cols] ) %*% coefi
mse.train[i] = mean( (Y.train - Y.pred)^2 )
}
i = 3
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)[-1]
A = data.matrix( df.test[, x_cols] )
A %*% coefi
A[1:10, ]
class(A)
for (i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)[-1]
Y.pred = coefi[1] + data.matrix( df.test[, x_cols] ) %*% coefi[-1]
mse.train[i] = mean( (Y.train - Y.pred)^2 )
}
mse.train
coefi[-1]
coefi[1]
# ------------------ Exercise 10 ------------------
n <- 1000
p <- 20
# (a)
set.seed(1)
X = poly(rnorm(n), p, raw=T)
beta = rnorm(p) * sample(0:1, p, replace=T)
eps = rnorm(n)
Y = X %*% beta + eps
# (b)
n.train <- 100
train = sample(n, n.train, replace=F)
test = -(train)
X.train = X[train, ]
X.test = X[test, ]
Y.train = Y[train]
Y.test = Y[test]
df.train = data.frame(y=Y.train, x=X.train)
df.test = data.frame(y=Y.test, x=X.test)
df.full = data.frame(y=Y, x=X)
# (c)
library(leaps)
bestsub.model = regsubsets(y~., data=df.train, nvmax=p)
mse.train = rep(NA, p)
colnames(df.train) = colnames(df.train, prefix='x.')
i = 1
coefi = coefficients(bestsub.model, id=i)
coefi
x_cols = names(coefi)[-1]
x_cols
Y.pred = coefi[1] + data.matrix( df.test[, x_cols] ) %*% coefi[-1]
Y.pred
integer(3)
# ------------------ Exercise 10 ------------------
n <- 1000
p <- 20
# (a)
library(MASS)
set.seed(1)
X = mvrnorm(n, mu=integer(p), Sigma=diag(p))
beta = rnorm(p) * sample(0:1, p, replace=T)
eps = rnorm(n)
Y = X %*% beta + eps
# (b)
n.train <- 100
train = sample(n, n.train, replace=F)
test = -(train)
X.train = X[train, ]
X.test = X[test, ]
Y.train = Y[train]
Y.test = Y[test]
df.train = data.frame(y=Y.train, x=X.train)
df.test = data.frame(y=Y.test, x=X.test)
df.full = data.frame(y=Y, x=X)
# (c)
library(leaps)
bestsub.model = regsubsets(y~., data=df.train, nvmax=p)
mse.train = rep(NA, p)
colnames(df.train) = colnames(df.train, prefix='x.')
for (i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)[-1]
Y.pred = coefi[1] + data.matrix( df.test[, x_cols] ) %*% coefi[-1]
mse.train[i] = mean( (Y.train - Y.pred)^2 )
}
mse.train
plot(mse.train, type='l', xlab='model size', ylab='MSE')
# ------------------ Exercise 10 ------------------
n <- 1000
p <- 20
# (a)
library(MASS)
set.seed(1)
X = mvrnorm(n, mu=integer(p), Sigma=diag(p))
beta = rnorm(p) * sample(0:1, p, replace=T)
eps = rnorm(n)
Y = X %*% beta + eps
# (b)
n.train <- 100
train = sample(n, n.train, replace=F)
test = -(train)
X.train = X[train, ]
X.test = X[test, ]
Y.train = Y[train]
Y.test = Y[test]
df.train = data.frame(y=Y.train, x=X.train)
df.test = data.frame(y=Y.test, x=X.test)
df.full = data.frame(y=Y, x=X)
# (c)
library(leaps)
bestsub.model = regsubsets(y~., data=df.train, nvmax=p)
mse.train = rep(NA, p)
colnames(df.train) = colnames(df.train, prefix='x.')
for (i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)
Y.pred = coefi[1] + data.matrix( df.test[, x_cols] ) %*% coefi[-1]
mse.train[i] = mean( (Y.train - Y.pred)^2 )
}
plot(mse.train, type='l', xlab='Model Size', ylab='Training MSE')
x_cols
for (i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)[-1]
Y.pred = coefi[1] + data.matrix( df.test[, x_cols] ) %*% coefi[-1]
mse.train[i] = mean( (Y.train - Y.pred)^2 )
}
plot(mse.train, type='l', xlab='Model Size', ylab='Training MSE')
# (d)
colnames(df.test) = colnames(df.test, prefix='x.')
mse.test = rep(NA, p)
for ( i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)
Y.pred = coefi[1] + data.matrix(df.test[, x_cols]) %*% coefi[-1]
mse.test[i] = mean( (Y.pred - Y.test)^2 )
}
plot(mse.test, type='l', xlab='Model Size', ylab='Test MSE')
# (d)
colnames(df.test) = colnames(df.test, prefix='x.')
mse.test = rep(NA, p)
for ( i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)[-1]
Y.pred = coefi[1] + data.matrix(df.test[, x_cols]) %*% coefi[-1]
mse.test[i] = mean( (Y.pred - Y.test)^2 )
}
plot(mse.test, type='l', xlab='Model Size', ylab='Test MSE')
mse.train
# ------------------ Exercise 10 ------------------
n <- 1000
p <- 20
# (a)
library(MASS)
set.seed(1)
X = mvrnorm(n, mu=integer(p), Sigma=diag(p))
beta = rnorm(p) * sample(0:1, p, replace=T)
eps = rnorm(n)
Y = X %*% beta + eps
# (b)
n.train <- 100
train = sample(n, n.train, replace=F)
test = -(train)
X.train = X[train, ]
X.test = X[test, ]
Y.train = Y[train]
Y.test = Y[test]
df.train = data.frame(y=Y.train, x=X.train)
df.test = data.frame(y=Y.test, x=X.test)
df.full = data.frame(y=Y, x=X)
# (c)
library(leaps)
bestsub.model = regsubsets(y~., data=df.train, nvmax=p)
mse.train = rep(NA, p)
colnames(df.train) = colnames(df.train, prefix='x.')
for (i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)[-1]
Y.pred = coefi[1] + data.matrix( df.train[, x_cols] ) %*% coefi[-1]
mse.train[i] = mean( (Y.train - Y.pred)^2 )
}
plot(mse.train, type='l', xlab='Model Size', ylab='Training MSE')
# (d)
colnames(df.test) = colnames(df.test, prefix='x.')
mse.test = rep(NA, p)
for ( i in 1:p){
coefi = coefficients(bestsub.model, id=i)
x_cols = names(coefi)[-1]
Y.pred = coefi[1] + data.matrix(df.test[, x_cols]) %*% coefi[-1]
mse.test[i] = mean( (Y.pred - Y.test)^2 )
}
plot(mse.test, type='l', xlab='Model Size', ylab='Test MSE')
# (e)
which.min(mse.test)
# (f)
beta
# (e)
id.best = which.min(mse.test)       # 11
id.best
# (f)
cofi.best = coefficients(bestsub.model, id=id.best)
cofi.best
beta
(3+1)
names(beta)
names(beta) = colnames(df.full, prefix='x.')
colnames(beta) = colnames(df.full, prefix='x.')
names(beta) = colnames(df.full, prefix='x.')[-1]
beta
sqrt(4)
id = 3
coefi = coefficients(bestsub.model, id=i)
selected.name = names(coefi)[-1]
selected.coefi = coefi[-1]
reg[i] = sqrt( sum( (beta[selecte.name] - selected.coefi)^2 ) + sum( (beta[-selected.name])^2 ) )
reg[i] = sqrt( sum( (beta[selected.name] - selected.coefi)^2 ) + sum( (beta[-selected.name])^2 ) )
beta
selected.name
coefi = coefficients(bestsub.model, id=i)
coefi
i = 1
coefi = coefficients(bestsub.model, id=i)
coefi
selected.name = names(coefi)[-1]
selected.coefi = coefi[-1]
selected.name
beta[-(selected.name)]
beta
beta[-'x.1']
names(beta) %in% selected.name
reg[i] = sqrt( sum( (beta[selected.name] - selected.coefi)^2 ) + sum( (beta[-(names(beta) %in% selected.name)])^2 ) )
names(beta) = colnames(df.full, prefix='x.')[-1]
reg = rep(NA, p)
for ( i in 1:p ){
coefi = coefficients(bestsub.model, id=i)
selected.name = names(coefi)[-1]
selected.coefi = coefi[-1]
reg[i] = sqrt( sum( (beta[selected.name] - selected.coefi)^2 ) + sum( (beta[-(names(beta) %in% selected.name)])^2 ) )
}
plot(reg, type='l', xlab='Model Size')
which.min(reg)
.rs.restartR()
# ------------------ Exercise 11 ------------------
library(ISLR)
attach(Boston)
# ------------------ Exercise 11 ------------------
library(MASS)
attach(Boston)
names(Boston)
?Boston
Boston = na.omit(Boston)
# best subset
library(leaps)
?regsubsets
bs.model = regsubsets(crim~., data=Boston, subset=train, method='exhaustive')
# (a)
set.seed(1)
train = sample(nrow(Boston), nrow(Boston)/2)
test = -(train)
bs.model = regsubsets(crim~., data=Boston, subset=train, method='exhaustive')
plot(bs.model)
dim(Boston)
bs.model = regsubsets(crim~., data=Boston, subset=train, nvmax=14, method='exhaustive')
plot(bs.model)
summary(bs.model)
s = summary(bs.model)
s$bic
bs.summary = summary(bs.model)
which.min(bs.summary$cp)
wichi.min(bs.summary$bic)
which.min(bs.summary$bic)
which.min(bs.summary$adjr2)
which.max(bs.summary$adjr2)
# hard to decide, use graphs
plot(bs.summary$cp, xlab='model size', ylab='Cp', type='l')
plot(bs.summary$bic, xlab='model size', ylab='BIC', type='l')
plot(bs.summary$adjr2, xlab='model size', ylab='Adj-R2', type='l')
# use 8
coefi = coefficients(bs.model, id=8)
coefi
# test error
x_cols = names(coefi)
X.test = Boston[test, x_cols]
# test error
x_cols = names(coefi)[-1]
x_cols
X.test = Boston[test, x_cols]
Y.test = Boston$crim[test]
class(X.test)
Y.pred = coefi[1] + data.matrix(X.test) %*% coefi[-1]
bs.test.mse = mean( (Y.pred - Y.test)^2 )
bs.test.mse
# forward selection
fs.model = regsubsets(crim~., data=Boston, subset=train, nvmax=14, method='forward')
# ------------------ Exercise 11 ------------------
library(MASS)
attach(Boston)
Boston = na.omit(Boston)
# (a)
set.seed(1)
train = sample(nrow(Boston), nrow(Boston)/2)
test = -(train)
# best subset
library(leaps)
bs.model = regsubsets(crim~., data=Boston, subset=train, nvmax=14, method='exhaustive')
bs.summary = summary(bs.model)
which.min(bs.summary$cp)        # 8
which.min(bs.summary$bic)       # 2
which.max(bs.summary$adjr2)     # 8
# hard to decide, use graphs
plot(bs.summary$cp, xlab='model size', ylab='Cp', type='l')
plot(bs.summary$bic, xlab='model size', ylab='BIC', type='l')
plot(bs.summary$adjr2, xlab='model size', ylab='Adj-R2', type='l')
# test error
x_cols = names(coefi)[-1]
X.test = Boston[test, x_cols]
Y.test = Boston$crim[test]
Y.pred = coefi[1] + data.matrix(X.test) %*% coefi[-1]
bs.test.mse = mean( (Y.pred - Y.test)^2 )    # 39.56733
# use 8
coefi = coefficients(bs.model, id=8)
# test error
x_cols = names(coefi)[-1]
X.test = Boston[test, x_cols]
Y.test = Boston$crim[test]
Y.pred = coefi[1] + data.matrix(X.test) %*% coefi[-1]
bs.test.mse = mean( (Y.pred - Y.test)^2 )    # 39.56733
# forward selection
fs.model = regsubsets(crim~., data=Boston, subset=train, nvmax=14, method='forward')
fs.summary = summary(fs.model)
which.min(fs.summary$cp)
which.min(fs.summary$bic)
which.max(fs.summary$adjr2)
# same situation, use 8
coefi = coefficients(fs.model, id=8)
coefi
X.test = Boston[test, x_cols]
Y.pred = coefi[1] + data.matrix(X.test) %*% coefi
Y.pred = coefi[1] + data.matrix(X.test) %*% coefi[-1]
fs.test.mse = mean( (Y.test - Y.pred)^2 )
fs.test.mse
library(MASS)
attach(Boston)
Boston = na.omit(Boston)
# (a)
set.seed(1)
train = sample(nrow(Boston), nrow(Boston)/2)
test = -(train)
# best subset
library(leaps)
fs.model = regsubsets(crim~., data=Boston, subset=train, nvmax=14, method='forward')
fs.summary = summary(fs.model)
which.min(fs.summary$cp)
which.min(fs.summary$bic)
which.max(fs.summary$adjr2)
# same situation, use 8
coefi = coefficients(fs.model, id=8)
# (Intercept)           zn          nox           rm          dis          rad      ptratio
# 13.49137141   0.04686838 -17.03403009   1.57135072  -1.29059959   0.55947948  -0.41356877
# lstat         medv
# 0.17036943  -0.26409286
x_cols = names(coefi)[-1]
X.test = Boston[test, x_cols]
Y.test = Boston$crim[test]
Y.pred = coefi[1] + data.matrix(X.test) %*% coefi[-1]
fs.test.mse = mean( (Y.test - Y.pred)^2 )      # 39.56733
# backward selection
bks.model = regsubsets(crim~., data=Boston, subset=train, method='backward')
bks.summary = summary(bks.model)
which.min(bks.summary$cp)
which.min(bks.summary$bic)
which.min(bks.summary$adjr2)
which.max(bks.summary$adjr2)
# the same, use 8
coefi = coefficients(bks.model, id=8)
coefi
x_cols = names(coefi)
x_cols = names(coefi)[-1]
Y.pred = coefi[1] + data.matrix(X.test) %*% coefi[-1]
bks.test.mse = mean( (Y.pred - Y.test)^2 )
bks.test.mse
# II.1 Lasso
library(glmnet)
?glmnet
X.train = Boston[train, -1]
Y.train = Boston$crim[train]
lasso.model = glmnet(data.matrix(X.train), Y.train, alpha=1)
summary(lasso.model)
cv.lasso = cv.glmnet(data.matrix(X.train), Y.train, alpha=1)
best.lambda = cv.lasso$lambda.min
best.lambda
X.test = Boston[test, -1]
Y.test = Boston$crim[test]
lasso.pred = predict(cv.lasso, s=best.lambda, newx=data.matrix(X.test), type='response')
lasso.mse = mean( (Y.pred - lasso.pred)^2 )
lasso.mse
lasso.model = predict(cv.lasso, s=best.lambda, newx=data.matrix(X.test), type='cofficients')
lasso.model = predict(cv.lasso, s=best.lambda, newx=data.matrix(X.test), type='cofficient')
lasso.model = predict(cv.lasso, s=best.lambda, newx=data.matrix(X.test), type='coefficients')
lasso.model
predict(bks.model, id=3)
predict(bks.model, Boston[1:100, ], id=3)
lasso.pred
lasso.mse = mean( (Y.test - lasso.pred)^2 )     # 0.3674139
lasso.mse
# II.2 Ridge
cv.ridge = glmnet(data.matrix(X.train), Y.train, alpha=0)
# II.2 Ridge
cv.ridge = cv.glmnet(data.matrix(X.train), Y.train, alpha=0)
best.lambda = cv.ridge$lambda.min
best.lambda
ridge.pred = predict(cv.ridge, newx=data.matrix(X.test), s=best.lambda, type='response')
ridge.mse = mean( (Y.test - ridge.pred)^2 )
ridge.mse
ridge.model = predict(cv.ridge, newx=data.matrix(X.test), s=best.lambda, type='coefficients')
ridge.model
# III. PCR
library(pls)
?pcr
pls.model = pcr(crim~., data=Boston, subset=train, scale=T, validation='CV')
validationplot(pls.model)
pls.model$ncomp
summary(pls.model)
summary(pls.model)
pls.pred = predict(pls.model, newdata=Boston[test, -1], ncomp=4)
pls.pred = predict(pls.model, newdata=Boston[test, -1], ncomp=4, type='response')
pcr.model = pcr(crim~., data=Boston, subset=train, scale=T, validation='CV')
validationplot(pls.model)
summary(pls.model)     # choose ncomp=4
pcr.pred = predict(pls.model, newdata=Boston[test, -1], ncomp=4, type='response')
pcr.mse = mean( (Y.test - pcr.pred)^2 )
pcr.mse
pcr.pred = predict(pls.model, newdata=Boston[test, -1], ncomp=10, type='response')
pcr.mse = mean( (Y.test - pcr.pred)^2 )    # 40.46918
pcr.mse
pcr.pred = predict(pls.model, newdata=Boston[test, -1], ncomp=13, type='response')
pcr.mse = mean( (Y.test - pcr.pred)^2 )    # 40.46918
pcr.mse
# Re-fit the model on the full dataset:
best.lambda = cv.lasso$lambda.min
lasso.fit = glmnet(data.matrix(Boston[, -1]), Boston$crim, alpha=1)
lasso.fit
lasso.model = predict(lasso.fit, type='response')
lasso.fit = glmnet(data.matrix(Boston[, -1]), Boston$crim, alpha=1)
summary(lasso.fit)
?glmnet
lasso.fit = glmnet(data.matrix(Boston[, -1]), lambda=best.lambda, Boston$crim, alpha=1)
lasso.fit
summary(lasso.fit)
coefficients(lasso.fit)
coefi = coefficients(lasso.fit)
coefi
