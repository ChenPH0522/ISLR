cols = rainbow(max(mtry.range))
x.lim = c(min(mtry.range), max(mtry.range))
y.lim = c(min(rss.mat), max(rss.mat))
plot(ntree.range, rss.mat[, 1], type='l', col=cols[1], xlim=x.lim, ylim=y.lim, xlab='number of trees', ylab='rss')
for(m in mtry.range[-1]){
lines(rss.mat[, m], col=cols[m])
}
legend('topright', legend=lab, col=cols, lwd=1, y.intersp=0.75)
lab = paste('m =', mtry.range)
cols = rainbow(max(mtry.range))
x.lim = c(min(mtry.range), max(mtry.range))
y.lim = c(min(rss.mat), max(rss.mat))
plot(ntree.range, rss.mat[, 1], type='l', col=cols[1], xlim=x.lim, ylim=y.lim, xlab='number of trees', ylab='rss')
for(m in mtry.range[-1]){
lines(rss.mat[, m], col=cols[m])
}
legend('topright', legend=lab, col=cols, lwd=1, x.intersp=0.5, y.intersp=0.75)
err.rate <- function(pm1){
pm2 = 1 - pm1
err = 1 - pmax(pm1, pm2)
return( err )
}
gini <- function(pm1){
pm2 = 1 - pm1
gini = pm1*(1-pm1) + pm2*(1-pm2)
return( gini )
}
cross.entropy <- function(pm1){
pm2 = 1 - pm1
c.e = -1*( pm1*log(pm1) + pm2*log(pm2) )
return( c.e )
}
pm1 = seq(0, 1, length.out=100)
e.r = err.rate(pm1)
g = gini(pm1)
c.e = cross.entropy(pm1)
x.lim = c(min(pm1), max(pm1))
y.lim = c(min(e.r, g, c.e, na.rm=TRUE), max(e.r, g, c.e, na.rm=TRUE))
plot(pm1, e.r, type='l', col='green', xlim=x.lim, ylim=y.lim, xlab='pm1', ylab='index value')
lines(pm1, g, col='red')
lines(pm1, c.e, col='blue')
legend('topright', legend=c('Classification Error', 'Gini', 'Cross Entropy'), col=c('green', 'red', 'blue'), lwd=1)
attach(Carseats)
library(ISLR)
attach(Carseats)
mse.mat = readRDS('rss_mat.rds')
lab = paste('m =', mtry.range)
cols = rainbow(max(mtry.range))
x.lim = c(min(mtry.range), max(mtry.range))
y.lim = c(min(mse.mat), max(mse.mat))
plot(ntree.range, mse.mat[, 1], type='l', col=cols[1], xlim=x.lim, ylim=y.lim, xlab='number of trees', ylab='mse')
for(m in mtry.range[-1]){
lines(mse.mat[, m], col=cols[m])
}
legend('topright', legend=lab, col=cols, lwd=1, x.intersp=0.5, y.intersp=0.75)
mtry.range = 1:14
ntree.range = 1:500
lab = paste('m =', mtry.range)
cols = rainbow(max(mtry.range))
x.lim = c(min(mtry.range), max(mtry.range))
y.lim = c(min(mse.mat), max(mse.mat))
plot(ntree.range, mse.mat[, 1], type='l', col=cols[1], xlim=x.lim, ylim=y.lim, xlab='number of trees', ylab='mse')
for(m in mtry.range[-1]){
lines(mse.mat[, m], col=cols[m])
}
legend('topright', legend=lab, col=cols, lwd=1, x.intersp=0.5, y.intersp=0.75)
lab = paste('m =', mtry.range)
cols = rainbow(max(mtry.range))
x.lim = c(min(mtry.range), max(mtry.range))
y.lim = c(min(mse.mat), max(mse.mat))
plot(ntree.range, mse.mat[, 1], type='l', col=cols[1], xlim=x.lim, ylim=y.lim, xlab='number of trees', ylab='mse')
for(m in mtry.range[-1]){
lines(mse.mat[, m], col=cols[m])
}
legend('topright', legend=lab, col=cols, lwd=1, x.intersp=0.5, y.intersp=0.75)
library(tree)
?Carseats
library(tree)
library(ISLR)
attach(Carseats)
set.seed(1)
# ------------------ (a) ------------------
idx.train = sample(nrow(Carseats), nrow(Carseats)%/%2)
train = Carseats[idx.train, ]
test = Carseats[-idx.train, ]
# ------------------ (b) ------------------
tree.fit = tree(Sales~., data=train)
tree.pred = predict(tree.fit, newdata=test)
tree.mse = mean( (tree.pred-test$Sales)^2 )
plot(tree.fit)
text(tree.fit, pretty=0)
plot(tree.fit)
text(tree.fit, pretty=0)
tree.mse
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit, FUN=prune.misclass)
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit)
prune.fit
prune.fit
prune.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(prune.fit$size, prune.fit$dev, type='l', xlab='tree size', ylab='deviance')
plot(prune.fit$k, prune.fit$dev, type='l', ylab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(prune.fit$size, prune.fit$dev, type='l', xlab='tree size', ylab='deviance')
plot(prune.fit$k, prune.fit$dev, type='l', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(prune.fit$size, prune.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(prune.fit$k, prune.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
tree.pred = predict(tree.fit, newdata=test)
library(tree)
library(ISLR)
attach(Carseats)
set.seed(1)
# ------------------ (a) ------------------
idx.train = sample(nrow(Carseats), nrow(Carseats)%/%2)
train = Carseats[idx.train, ]
test = Carseats[-idx.train, ]
# ------------------ (b) ------------------
tree.fit = tree(Sales~., data=train)
tree.pred = predict(tree.fit, newdata=test)
tree.mse = mean( (tree.pred-test$Sales)^2 )    # MSE = 4.922039
plot(tree.fit)
text(tree.fit, pretty=0)
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(prune.fit$size, prune.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(prune.fit$k, prune.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(prune.fit$size, prune.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(prune.fit$k, prune.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(prune.fit$size, prune.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(prune.fit$k, prune.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(prune.fit$size, prune.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(prune.fit$k, prune.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(prune.fit$size, prune.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(prune.fit$k, prune.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(prune.fit$size, prune.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(prune.fit$k, prune.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
prune.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(prune.fit$size, prune.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(prune.fit$k, prune.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
prune.fit
which.min(prune.fit$dev)
# we choose size=12
prune.fit = prune.tree(cv.fit, best=12)
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )
# we choose size=12
prune.fit = prune.tree(prune.fit, best=12)
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )
# we choose size=12
prune.fit = prune.tree(prune.fit, best=12)
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )
prune.fit = prune.tree(prune.fit, best=12)
cv.fit = cv.tree(tree.fit)
# we choose size=12
prune.fit = prune.tree(prune.fit, best=12)
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# we choose size=12
prune.fit = prune.tree(cv.fit, best=12)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )
rm(prune.fit)
# we choose size=12
prune.fit = prune.tree(cv.fit, best=12)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )
cv.fit
# we choose size=12
prune.fit = prune.tree(cv.fit, best=8)
cv.fit
prune.fit = prune.tree(cv.fit, k=30, best=8)
# we choose size=12
prune.fit = prune.tree(cv.fit)
# we choose size=12
prune.fit = prune.tree(cv.fit, best = 8)
library(tree)
library(ISLR)
attach(Carseats)
set.seed(1)
# ------------------ (a) ------------------
idx.train = sample(nrow(Carseats), nrow(Carseats)%/%2)
train = Carseats[idx.train, ]
test = Carseats[-idx.train, ]
# ------------------ (b) ------------------
tree.fit = tree(Sales~., data=train)
tree.pred = predict(tree.fit, newdata=test)
tree.mse = mean( (tree.pred-test$Sales)^2 )    # MSE = 4.922039
plot(tree.fit)
text(tree.fit, pretty=0)
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# we choose size=12
prune.fit = prune.tree(cv.fit, best = 8)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit, FUN=prune.tree)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# we choose size=12
prune.fit = prune.tree(cv.fit, best = 8)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )
# we choose size=12
prune.fit = prune.tree(tree.fit, best = 8)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )
prune.mse
library(tree)
library(ISLR)
attach(Carseats)
set.seed(1)
# ------------------ (a) ------------------
idx.train = sample(nrow(Carseats), nrow(Carseats)%/%2)
train = Carseats[idx.train, ]
test = Carseats[-idx.train, ]
# ------------------ (b) ------------------
tree.fit = tree(Sales~., data=train)
tree.pred = predict(tree.fit, newdata=test)
tree.mse = mean( (tree.pred-test$Sales)^2 )    # MSE = 4.922039
plot(tree.fit)
text(tree.fit, pretty=0)
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit, FUN=prune.tree)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# we choose size=12
prune.fit = prune.tree(tree.fit, best = 8)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )
prune.mse
cv.fit
# we choose size=12
prune.fit = prune.tree(tree.fit, best = 14)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )    # MSE =
prune.mse
# we choose size=12
prune.fit = prune.tree(tree.fit, best = 18)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )    # MSE =
prune.mse
library(tree)
library(ISLR)
attach(Carseats)
set.seed(1)
# ------------------ (a) ------------------
idx.train = sample(nrow(Carseats), nrow(Carseats)%/%2)
train = Carseats[idx.train, ]
test = Carseats[-idx.train, ]
# ------------------ (b) ------------------
tree.fit = tree(Sales~., data=train)
tree.pred = predict(tree.fit, newdata=test)
tree.mse = mean( (tree.pred-test$Sales)^2 )    # MSE = 4.922039
plot(tree.fit)
text(tree.fit, pretty=0)
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit, FUN=prune.tree)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# we choose size=18
prune.fit = prune.tree(tree.fit, best = 18)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )    # MSE = 4.922039
# Pruning the tree does not improve the test MSE in this case
# ------------------ (d) ------------------
ncol(Boston)
ncol(Carseats)
bag.fit = randomForest(medv~., data=train, mtry=10, importance=TRUE)
bag.pred = predict(bag.fit, newdata=test)
library(randomForest)
bag.fit = randomForest(medv~., data=train, mtry=10, importance=TRUE)
bag.pred = predict(bag.fit, newdata=test)
bag.fit = randomForest(Sales~., data=train, mtry=10, importance=TRUE)
bag.pred = predict(bag.fit, newdata=test)
library(randomForest)
library(tree)
library(ISLR)
attach(Carseats)
set.seed(1)
# ------------------ (a) ------------------
idx.train = sample(nrow(Carseats), nrow(Carseats)%/%2)
train = Carseats[idx.train, ]
test = Carseats[-idx.train, ]
# ------------------ (b) ------------------
tree.fit = tree(Sales~., data=train)
tree.pred = predict(tree.fit, newdata=test)
tree.mse = mean( (tree.pred-test$Sales)^2 )    # MSE = 4.922039
plot(tree.fit)
text(tree.fit, pretty=0)
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit, FUN=prune.tree)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# we choose size=18
prune.fit = prune.tree(tree.fit, best = 18)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )    # MSE = 4.922039
# Pruning the tree does not improve the test MSE in this case
# ------------------ (d) ------------------
bag.fit = randomForest(Sales~., data=train, mtry=10, importance=TRUE)
bag.pred = predict(bag.fit, newdata=test)
bag.mse = mean( (bag.pred-test$Sales)^2 )
bag.mse
importance(bag.fit)
varImpPlot(bag.fit)
sqrt(11)
View(test)
# ------------------ (e) ------------------
m.vec = 1:10
rss.vec = rep(0, max(m.vec))
for(m in m.vec){
rf.fit = randomForest(Sales~., data=train, mtry=m, importance=TRUE)
rf.pred = predict(rf.fit, newdata=test)
rss.vec[m] = mean( (rf.pred-test$Sales)^2 )
}
plot(m.vec, rss.vec, type='b', xlab='m', ylab='rss')
which.min(rss.vec)
library(randomForest)
library(tree)
library(ISLR)
attach(Carseats)
set.seed(1)
# ------------------ (a) ------------------
idx.train = sample(nrow(Carseats), nrow(Carseats)%/%2)
train = Carseats[idx.train, ]
test = Carseats[-idx.train, ]
# ------------------ (b) ------------------
tree.fit = tree(Sales~., data=train)
tree.pred = predict(tree.fit, newdata=test)
tree.mse = mean( (tree.pred-test$Sales)^2 )    # MSE = 4.922039
plot(tree.fit)
text(tree.fit, pretty=0)
# ------------------ (c) ------------------
cv.fit = cv.tree(tree.fit, FUN=prune.tree)
par(mfrow=c(1, 2))
plot(cv.fit$size, cv.fit$dev, type='b', xlab='tree size', ylab='deviance')
plot(cv.fit$k, cv.fit$dev, type='b', xlab='tuning parameter', ylab='deviance')
# we choose size=18
prune.fit = prune.tree(tree.fit, best = 18)
par(mfrow=c(1,1))
plot(prune.fit)
text(prune.fit, pretty=0)
prune.pred = predict(prune.fit, newdata=test)
prune.mse = mean( (prune.pred-test$Sales)^2 )    # MSE = 4.922039
# Pruning the tree does not improve the test MSE in this case
# ------------------ (d) ------------------
bag.fit = randomForest(Sales~., data=train, mtry=10, importance=TRUE)
bag.pred = predict(bag.fit, newdata=test)
bag.mse = mean( (bag.pred-test$Sales)^2 )    # MSE = 2.657296
importance(bag.fit)
varImpPlot(bag.fit)
# Price and ShelveLoc are the most important 2 factors
# ------------------ (e) ------------------
m.vec = 1:10
mse.vec = rep(0, max(m.vec))
for(m in m.vec){
rf.fit = randomForest(Sales~., data=train, mtry=m, importance=TRUE)
rf.pred = predict(rf.fit, newdata=test)
mse.vec[m] = mean( (rf.pred-test$Sales)^2 )
}
plot(m.vec, mse.vec, type='b', xlab='m', ylab='rss')
# The m that leads to the minimum rss is 7
# The minimum
min(mse.vec)
